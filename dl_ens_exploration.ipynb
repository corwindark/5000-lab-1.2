{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "l14ED15pN7TW",
        "N3FbBbIBN-db"
      ],
      "authorship_tag": "ABX9TyMGriX+CzDnOIInxXjr06f6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corwindark/5000-lab-1.2/blob/main/dl_ens_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Setup - Run Every Time"
      ],
      "metadata": {
        "id": "IcQ0TfR_NvtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GmcHy4PefJh",
        "outputId": "ac0e5f19-4182-4121-d936-c81e7e288e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TENSORFLOW VERSION: 2.15.0\n",
            "PYTORCH VERSION: 2.1.0+cu121\n",
            "KERAS VERSION: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "# TENSORFLOW\n",
        "import tensorflow as tf\n",
        "print(\"TENSORFLOW VERSION:\",tf.__version__)\n",
        "\n",
        "# PYTORCH\n",
        "import torch\n",
        "print(\"PYTORCH VERSION:\",torch.__version__)\n",
        "\n",
        "# KERAS\n",
        "import keras;\n",
        "print(\"KERAS VERSION:\",keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgLd74Zteqck",
        "outputId": "894f9395-e134-46d5-9df4-79f04d00c125"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET SYSTEM INFO\n",
        "# code modified from: https://stackoverflow.com/questions/110362/how-can-i-find-the-current-os-in-python\n",
        "\n",
        "import platform\n",
        "import multiprocessing\n",
        "import sys\n",
        "import psutil\n",
        "\n",
        "def linux_distribution():\n",
        "  try:\n",
        "    return platform.linux_distribution()\n",
        "  except:\n",
        "    return \"N/A\"\n",
        "\n",
        "def dist():\n",
        "  try:\n",
        "    return platform.dist()\n",
        "  except:\n",
        "    return \"N/A\"\n",
        "\n",
        "num_cores=multiprocessing.cpu_count()\n",
        "\n",
        "print(\"\"\"\n",
        "Python version: %s\n",
        "dist: %s\n",
        "num_cores: %s\n",
        "linux_distribution: %s\n",
        "system: %s\n",
        "machine: %s\n",
        "platform: %s\n",
        "uname: %s\n",
        "version: %s\n",
        "RAM: %s\n",
        "\"\"\" % (\n",
        "sys.version.split('\\n'),\n",
        "str(dist()),\n",
        "num_cores,\n",
        "linux_distribution(),\n",
        "platform.system(),\n",
        "platform.machine(),\n",
        "platform.platform(),\n",
        "platform.uname(),\n",
        "platform.version(),\n",
        "psutil.virtual_memory().total*10**(-9.)\n",
        "))\n",
        "\n",
        "# print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWIeaqgue6WW",
        "outputId": "522e3a51-5472-4140-ea2f-1a68691d1544"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Python version: ['3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]']\n",
            "dist: N/A\n",
            "num_cores: 2\n",
            "linux_distribution: N/A\n",
            "system: Linux\n",
            "machine: x86_64\n",
            "platform: Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "uname: uname_result(system='Linux', node='d6fd48fcc495', release='6.1.58+', version='#1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023', machine='x86_64')\n",
            "version: #1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\n",
            "RAM: 13.60945152\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()\n",
        "torch.cuda.get_device_properties(0).total_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHpcauGQfDuU",
        "outputId": "b7d71775-ddcd-4e59-b2ff-585c1c859836"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15835660288"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ],
      "metadata": {
        "id": "-YU0DRxuga_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcfcd1f-d70a-4a09-b212-efee766b955b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/yfinance/base.py:48: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  _empty_series = pd.Series()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsforecast\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKOyzoq0kuAB",
        "outputId": "1222689e-4d25-44e0-958c-11a91ca9a0df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import AutoCES, AutoARIMA, AutoETS, DynamicOptimizedTheta\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI2srljshLUI",
        "outputId": "27e3c9cc-fa0f-4b1c-c707-1d45b2776c29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Stat Functions"
      ],
      "metadata": {
        "id": "l14ED15pN7TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crossValidation(train, test, window, outputFrame, modelName, predReturnFunction):\n",
        "\n",
        "    totalPredictions = pd.Series(dtype = 'float64')\n",
        "    windows = len(test) // window\n",
        "\n",
        "    predictionIndex = 0\n",
        "\n",
        "    for i in range(0, windows):\n",
        "        # debug\n",
        "        #print(\"Window: \", i, \"Method: \", modelName)\n",
        "\n",
        "        # How many observations to move forward each frame\n",
        "        addNum = i * window\n",
        "        # Combine training data with additional test window\n",
        "        intermediateData = pd.concat([train, test[:addNum]])\n",
        "        # Generate prediction of the given window size\n",
        "        prediction = predReturnFunction(intermediateData, window)\n",
        "        #print(prediction)\n",
        "        # Check if we have multiple predictions in the window\n",
        "        if len(prediction) > 1:\n",
        "            # Store each predicted value with a loop\n",
        "            for j in range(0,window):\n",
        "\n",
        "                # Store in the prediction-comparison frame\n",
        "                outputFrame[modelName][predictionIndex] = prediction[j]\n",
        "\n",
        "                # Move to next open spot\n",
        "                predictionIndex += 1\n",
        "        else:\n",
        "\n",
        "            outputFrame[modelName][predictionIndex] = prediction[0]\n",
        "            predictionIndex += 1\n"
      ],
      "metadata": {
        "id": "HVpcUSSjhJOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stat_test_wrapper(seriesTrain, seriesTest, testDF,  test_out_size = 100, window_size = 1):\n",
        "    # Set the number of observations to be included in the test set\n",
        "\n",
        "  testDFCopy = testDF.copy()\n",
        "\n",
        "  tempTest = seriesTest[:test_out_size]\n",
        "\n",
        "  # Auto Arima\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'auto_arima', aaPredFunction)\n",
        "\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'complex_smoothing', cesPredFunction)\n",
        "\n",
        "  #crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'auto_ets', etsPredFunction)\n",
        "\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'dyn_theta', dotPredFunction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plot2 = testDFCopy.iloc[0:test_out_size,:]\n",
        "\n",
        "  preds = plot2.loc[:,['auto_arima', 'complex_smoothing', 'dyn_theta']]\n",
        "  # ensemble methods\n",
        "  plot2.loc[:,'mean_ens'] = preds.mean(axis = 1)\n",
        "  plot2.loc[:,'median_ens'] = preds.median(axis = 1)\n",
        "\n",
        "\n",
        "  plot2.loc[:,'auto_arima'] = abs( plot2.loc[:,'auto_arima'] - plot2.loc[:,'Price'] )\n",
        "  # auto ets is not very good apparently\n",
        "  #plot2.loc[:,'auto_ets'] = abs( plot2.loc[:,'auto_ets'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'complex_smoothing'] = abs( plot2.loc[:,'complex_smoothing'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'dyn_theta'] = abs( plot2.loc[:,'dyn_theta'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'mean_ens'] = abs( plot2.loc[:,'mean_ens'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'median_ens'] = abs( plot2.loc[:,'median_ens'] - plot2.loc[:,'Price'] )\n",
        "\n",
        "\n",
        "\n",
        "  # get the index as a column for plotting\n",
        "  plot2 = plot2.reset_index()\n",
        "\n",
        "  errorDat = plot2.copy()\n",
        "\n",
        "  names = ['auto_arima', 'dyn_theta', 'complex_smoothing']\n",
        "  errorDat['optimal'] = errorDat[names].idxmin(axis=\"columns\")\n",
        "\n",
        "  #print(\"reached loop\")\n",
        "  \"\"\"\n",
        "  for index in range(0, errorDat.shape[0]):\n",
        "      #print(index)\n",
        "\n",
        "      predictionErrors = errorDat.loc[index,['auto_arima', 'auto_ets', 'dyn_theta', 'complex_smoothing'] ]\n",
        "\n",
        "      #print(predictionErrors)\n",
        "\n",
        "      #print(min(abs(predictionErrors)))\n",
        "      #errorDat.loc[index ,'optimal'] = min(abs(predictionErrors))\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  return testDFCopy, errorDat"
      ],
      "metadata": {
        "id": "e0eVuMiIlN01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aaPredFunction(dataIn, windowSize):\n",
        "    model = AutoARIMA()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    prediction = fit1.predict(h = windowSize)\n",
        "    return prediction.get('mean')\n",
        "\n",
        "\n",
        "def cesPredFunction(dataIn, windowSize):\n",
        "    model = AutoCES()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def etsPredFunction(dataIn, windowSize):\n",
        "    model = AutoETS()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def dotPredFunction(dataIn, windowSize):\n",
        "    model = DynamicOptimizedTheta()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "\n",
        "\n",
        "# Set the number of observations to be included in the test set\n",
        "test_out_size = 200\n",
        "\n",
        "tempTest = seriesTest[:test_out_size]\n"
      ],
      "metadata": {
        "id": "tM9QbHylhSvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prep_ticker(ticker, start_date = '2022-06-01', end_date = '2023-09-30', intervals = '60m', split = True, train_size = 2000, model_list = ['auto_arima', 'dyn_theta', 'auto_ets', 'complex_smoothing']):\n",
        "  data = yf.download(ticker,start_date, end_date, interval = intervals)\n",
        "  #print(data.head())\n",
        "  data.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "  data_train = data[:train_size]\n",
        "  data_test = data[train_size:]\n",
        "\n",
        "  test_size = len(data_test)\n",
        "\n",
        "\n",
        "  data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "  data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "  seriesTrain = data_train.set_index('Datetime')\n",
        "  seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "  testResultsDF = pd.DataFrame(index = range(test_size))\n",
        "  testResultsDF['Price'] = 0\n",
        "\n",
        "  #print(data.head())\n",
        "\n",
        "  # Get a clean format for close prices\n",
        "  for i in range(0,test_size):\n",
        "      testResultsDF['Price'][i] = seriesTest.values[i][0]\n",
        "\n",
        "\n",
        "  # Initialize empty cells for the statistical forecasts\n",
        "  for modeltype in model_list:\n",
        "    testResultsDF[modeltype] = 0\n",
        "\n",
        "  return testResultsDF, seriesTrain, seriesTest"
      ],
      "metadata": {
        "id": "GRRsuTtaWZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Stat Examples\n"
      ],
      "metadata": {
        "id": "N3FbBbIBN-db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = yf.download('SPY','2022-06-01','2023-09-30', interval =\"60m\")\n",
        "%matplotlib inline\n",
        "\n",
        "data['Adj Close'].plot()\n",
        "plt.ion()\n",
        "print(data)\n",
        "\n",
        "data.reset_index(inplace = True)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "yzKoe4nigzre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 2000\n",
        "\n",
        "data_train = data[:train_size]\n",
        "data_test = data[train_size:]\n",
        "\n",
        "test_size = len(data_test)\n",
        "\n",
        "\n",
        "data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "seriesTrain = data_train.set_index('Datetime')\n",
        "seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "cv_data = data[['Close']]"
      ],
      "metadata": {
        "id": "Xo_WQK4WhEky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reformat the test data into a dataframe we can add our predictions to\n",
        "\n",
        "testResultsDF = pd.DataFrame(index = range(test_size))\n",
        "testResultsDF['Price'] = 0\n",
        "# Get a clean format for close prices\n",
        "for i in range(0,test_size):\n",
        "    testResultsDF['Price'][i] = seriesTest.values[i][0]\n",
        "\n",
        "\n",
        "# Initialize empty cells for the statistical forecasts\n",
        "testResultsDF['auto_arima'] = 0\n",
        "testResultsDF['dyn_theta'] = 0\n",
        "#testResultsDF['auto_ets'] = 0\n",
        "testResultsDF['complex_smoothing'] = 0"
      ],
      "metadata": {
        "id": "nAYAKdiChGmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Auto Arima\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_arima', aaPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'complex_smoothing', cesPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_ets', etsPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'dyn_theta', dotPredFunction)\n"
      ],
      "metadata": {
        "id": "nzAn1r3eAO3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testResultsDF)\n",
        "\n",
        "plot1 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot1 = plot1.reset_index()\n",
        "\n",
        "plot1 = pd.melt(plot1, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta', 'Price'])\n",
        "\n",
        "print(plot1)\n",
        "\n",
        "sns.lineplot(plot1, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "3pR4l_YPhdoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot1 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot1 = plot1.reset_index()\n",
        "\n",
        "plot1 = pd.melt(plot1, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta', 'Price'])\n",
        "\n",
        "print(plot1)\n",
        "\n",
        "sns.lineplot(plot1, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "99el4haXjZJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testResultsDF)\n",
        "\n",
        "plot2 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "plot2['auto_arima'] = plot2['auto_arima'] - plot2['Price']\n",
        "plot2['auto_ets'] = plot2['auto_ets'] - plot2['Price']\n",
        "plot2['complex_smoothing'] = plot2['complex_smoothing'] - plot2['Price']\n",
        "plot2['dyn_theta'] = plot2['dyn_theta'] - plot2['Price']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot2 = plot2.reset_index()\n",
        "\n",
        "errorDat = plot2.copy()\n",
        "\n",
        "plot2 = pd.melt(plot2, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta'])\n",
        "\n",
        "sns.barplot(plot2, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "P-p9QbHCj5gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(AAPLerrors.head())\n",
        "print(AAPLerrors['optimal'].value_counts() )\n",
        "\n",
        "print(AAPLerrors['auto_arima'].mean())\n",
        "print(AAPLerrors['dyn_theta'].mean())\n",
        "print(AAPLerrors['complex_smoothing'].mean())\n",
        "print(AAPLerrors['mean_ens'].mean())\n",
        "\n",
        "optimals = ['auto_arima', 'dyn_theta', 'complex_smoothing']\n",
        "print(AAPLerrors[optimals].min(axis=\"columns\").mean())\n"
      ],
      "metadata": {
        "id": "aeAJVba9zC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AAPLpreds, AAPLerrors = stat_test_wrapper(trainSeriesAAPL, testSeriesAAPL, testResultsAAPL, 25, 1)\n"
      ],
      "metadata": {
        "id": "G57EBqz8y2S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errorDat['optimal'] = 0\n",
        "\n",
        "for index in range(0, errorDat.shape[0]):\n",
        "   # print(index)\n",
        "    predictionErrors = errorDat.loc[:,('auto_arima', 'auto_ets', 'dyn_theta', 'complex_smoothing') ]\n",
        "\n",
        "    #print(predictionErrors.min())\n",
        "\n",
        "    errorDat.loc[index ,'optimal'] = min(predictionErrors)\n",
        "\n",
        "errorDat"
      ],
      "metadata": {
        "id": "rRxuc2fUly-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qqqerrors.head())\n",
        "print(qqqerrors['optimal'].value_counts() )\n",
        "# QQQ, 22-6-1 to 23-9-30, 1hr,\n",
        "# > 5-1 is auto_arima 4, complex smoothing 1\n",
        "# > 100-1 is auto_arima 50, complex smoothing 44, dyn_theta 6\n",
        "# > 200-20 is auto_arima 41, complex_smoothing 111, dyn_theta 48\n",
        "# > 200-50 is auto_arima 18, c_s 92, dyn_theta 90\n",
        "# > 200-100 is a_a 60, c_s 78, d_t 62\n",
        "# AAPL, 22-6-1 to 23-9-30, 1hr,\n",
        "# 100 - 1 is a_a 42, c_s 51, d_t 7\n",
        "\n"
      ],
      "metadata": {
        "id": "5oF603FGsIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# New DL Functions"
      ],
      "metadata": {
        "id": "Cx2aXYcaOA0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting more different financial datasets\n",
        "\n",
        "\n",
        "def prep_ticker(ticker, start_date = '2022-06-01', end_date = '2023-09-30', intervals = '60m', split = True, train_size = 2000):\n",
        "  data = yf.download(ticker,start_date, end_date, interval = intervals)\n",
        "\n",
        "  #print(data.head)\n",
        "\n",
        "  data.reset_index(inplace = True)\n",
        "\n",
        "  if split == True:\n",
        "    data_train = data[:train_size]\n",
        "    data_test = data[train_size:]\n",
        "\n",
        "    test_size = len(data_test)\n",
        "\n",
        "\n",
        "    data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "    data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "    seriesTrain = data_train.set_index('Datetime')\n",
        "    seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "\n",
        "    return seriesTrain, seriesTest\n",
        "\n",
        "  elif split == False:\n",
        "    data_out = data\n",
        "\n",
        "    data_out = data_out[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "    seriesOut = data_out.set_index('Datetime')\n",
        "\n",
        "\n",
        "    return seriesOut\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZVIFG9onTH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_aaPredFunction(dataIn, windowSize):\n",
        "    model = AutoARIMA()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    prediction = fit1.predict(h = windowSize)\n",
        "    return prediction.get('mean')\n",
        "\n",
        "\n",
        "def new_cesPredFunction(dataIn, windowSize):\n",
        "    model = AutoCES()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def new_etsPredFunction(dataIn, windowSize):\n",
        "    model = AutoETS()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def new_dotPredFunction(dataIn, windowSize):\n",
        "    model = DynamicOptimizedTheta()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')"
      ],
      "metadata": {
        "id": "nzhZY8XEPjkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stat_true_window_cross_val(unsplitdata, model_names, predReturnFunctions, train_window = 10, test_window = 1, nn_task = 'regression'):\n",
        "    totalPredictions = pd.Series(dtype = 'float64')\n",
        "    windows = len(unsplitdata) // (train_window + test_window)\n",
        "    columns = train_window + (test_window * (1 + len(model_names)))\n",
        "\n",
        "    pred_df = np.zeros([windows, columns])\n",
        "    print(pred_df.shape)\n",
        "\n",
        "    unsplitdata = np.array(unsplitdata)\n",
        "    unsplitdata = unsplitdata.reshape((unsplitdata.shape[0],))\n",
        "    print(unsplitdata.shape)\n",
        "\n",
        "    # indexes loop through the data\n",
        "    predictionIndex = 0\n",
        "    startindex = 0\n",
        "\n",
        "    for i in range(0, windows):\n",
        "        # debug\n",
        "        #print(\"Window: \", i)\n",
        "\n",
        "        train_window_dat = unsplitdata[startindex:(startindex+train_window)]\n",
        "        #print(train_window_dat)\n",
        "        test_window_dat = unsplitdata[(startindex+train_window ): (test_window + startindex+train_window)]\n",
        "\n",
        "        startindex = startindex + train_window + test_window\n",
        "\n",
        "        #print(pred_df[i,0:train_window])\n",
        "        #print(pred_df[i,0:train_window].shape)\n",
        "\n",
        "        #print(train_window_dat.shape)\n",
        "        #print( train_window_dat.reshape((train_window,)).shape)\n",
        "\n",
        "        pred_df[i,0:train_window] = np.reshape(train_window_dat, [train_window])\n",
        "        pred_df[i,(train_window):(train_window + test_window)] = test_window_dat\n",
        "\n",
        "        for modnum, modelfunction in enumerate(predReturnFunctions):\n",
        "          # account for how many columns in output array over to shift for this model\n",
        "          # add 1 to fit the real values\n",
        "          #print(modnum)\n",
        "          offset = ((modnum + 1) * test_window) + train_window\n",
        "\n",
        "          # Generate prediction of the given window size\n",
        "          prediction = modelfunction(train_window_dat, test_window)\n",
        "          #print(prediction[0])\n",
        "\n",
        "          # Check if we have multiple predictions in the window\n",
        "          if len(prediction) > 1:\n",
        "              # Store each predicted value with a loop\n",
        "              for j in range(0,test_window):\n",
        "\n",
        "                  # Store in the prediction-comparison frame\n",
        "                  pred_df[i,j + offset] = prediction[j]\n",
        "\n",
        "          else:\n",
        "\n",
        "              pred_df[i,offset] = prediction[0]\n",
        "\n",
        "    # currently configured to split up data into 3 matrices instead of returning together\n",
        "    x_windows = pred_df[:,0:train_window]\n",
        "    real_ys = pred_df[:,train_window:(train_window + test_window)]\n",
        "    stat_preds = pred_df[:,(train_window + test_window):(columns+1) ]\n",
        "\n",
        "\n",
        "    #print(pred_df)\n",
        "\n",
        "    return x_windows, real_ys, stat_preds\n"
      ],
      "metadata": {
        "id": "skBWEjboSjvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def history_plot(history):\n",
        "    FS=18   #FONT SIZE\n",
        "    # PLOTTING THE TRAINING AND VALIDATION LOSS\n",
        "    history_dict = history.history\n",
        "    loss_values = history_dict[\"loss\"]\n",
        "    val_loss_values = history_dict[\"val_loss\"]\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "    plt.title(\"Training and validation loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ql8LVy1foPmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(window_size, real_data, stat_pred, nn_pred):\n",
        "    # declare values we will calculate when looping through the real data\n",
        "    stat_avg_mse, nn_mse, nn_avg_mse = 0,0,0\n",
        "    # find out how many stat models are in the ensemble\n",
        "    num_stat_mods = int(stat_pred.shape[1] / window_size)\n",
        "\n",
        "    print(num_stat_mods)\n",
        "\n",
        "    for row in range(0, real_data.shape[0]):\n",
        "      for obs in range(0,real_data.shape[1]):\n",
        "\n",
        "        # real value\n",
        "        rv = real_data[row,obs]\n",
        "\n",
        "        # squared error for neural network model\n",
        "        nn_mse += (rv - nn_pred[row,obs]) ** 2\n",
        "\n",
        "        # numerator for stat model avg\n",
        "        stat_numerator = 0\n",
        "\n",
        "        for mod in range(0,num_stat_mods):\n",
        "          # add each models prediction for the given datapoint\n",
        "          stat_numerator += stat_pred[row,obs + (mod*window_size)]\n",
        "\n",
        "        # numerator for the stat+NN ensemble\n",
        "        nn_stat_numerator = stat_numerator + nn_pred[row,obs]\n",
        "\n",
        "        # find the ensemble prediction for the datapoint\n",
        "        stat_avg = stat_numerator / num_stat_mods\n",
        "        nn_stat_avg = nn_stat_numerator / (num_stat_mods + 1)\n",
        "\n",
        "        # calculate the squared error for these ensembles\n",
        "        stat_avg_mse += (rv - stat_avg) ** 2\n",
        "        nn_avg_mse += (rv - nn_stat_avg) ** 2\n",
        "\n",
        "    # find mean square error instead of total\n",
        "    n_obs = real_data.shape[0]\n",
        "    nn_mse = nn_mse / n_obs\n",
        "    stat_avg_mse = stat_avg_mse / n_obs\n",
        "    nn_avg_mse = nn_avg_mse / n_obs\n",
        "\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse\n",
        "\n"
      ],
      "metadata": {
        "id": "VtAPlWf4owQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_single_nn_wrapper(\n",
        "    nn_modeltype,\n",
        "    nn_task,\n",
        "    train_split,\n",
        "    ticker,\n",
        "    model_names = ['auto_arima','complex_smoothing'],\n",
        "    predReturnFunctions = [new_aaPredFunction, new_cesPredFunction],\n",
        "    start_date_in = '2022-06-01',\n",
        "    end_date_in = '2023-09-30',\n",
        "    interval_in = '60m',\n",
        "    split = False,\n",
        "    train_size = 2000,\n",
        "    train_window = 10,\n",
        "    test_window = 1,\n",
        "    epochs_in = 10,\n",
        "    verbose = 1):\n",
        "\n",
        "  # use inputs to get stock data of the requested format\n",
        "  ticker_data = prep_ticker(ticker, intervals = interval_in, start_date = start_date_in, end_date = end_date_in, split = split)\n",
        "\n",
        "  # difference data to test if it makes a 'difference'\n",
        "  # update, differenced data seems to break auto CES\n",
        "  #ticker_data = ticker_data.diff()\n",
        "\n",
        "  # normalize data\n",
        "  ticker_mean = ticker_data.mean()\n",
        "  ticker_std = ticker_data.std()\n",
        "  ticker_data = (ticker_data - ticker_mean) / ticker_std\n",
        "\n",
        "  # break stock data into windows and return statistical model predictions\n",
        "  xs,ys,preds = stat_true_window_cross_val(ticker_data, model_names, predReturnFunctions, train_window, test_window)\n",
        "\n",
        "  # generate categorical outcome data (which model was optimal) if task is classification\n",
        "  # only works with 1-ahead windows for now\n",
        "  stat_model_number = len(model_names)\n",
        "  y_optimals = np.zeros(shape = [ys.shape[0], stat_model_number])\n",
        "\n",
        "  #iterate through the occurences to find optimal model in the ensemble in each\n",
        "  for i in range(0, y_optimals.shape[0]):\n",
        "    yval = ys[i,0]\n",
        "    errors = np.zeros(shape = preds[i,:].shape)\n",
        "    #print(\"error shape: \", errors.shape)\n",
        "    for j in range(0, errors.shape[0]):\n",
        "       errors[j] = yval - preds[i,j]\n",
        "\n",
        "    y_optimals[i, np.argmin(errors, axis=0)] = 1\n",
        "\n",
        "  #print(y_optimals)\n",
        "  if nn_task == \"classification\":\n",
        "    ys = y_optimals\n",
        "\n",
        "\n",
        "  # status update\n",
        "  if verbose == 1:\n",
        "    print(\"STAT MODELS TRAINED\")\n",
        "\n",
        "  # calculate train test split\n",
        "  train_obs = round(xs.shape[0] * train_split)\n",
        "\n",
        "\n",
        "\n",
        "  # divide matrices\n",
        "  x_train_dat = xs[0:train_obs,:]\n",
        "  x_val_dat = xs[train_obs:xs.shape[0],:]\n",
        "  y_train_dat = ys[0:train_obs,:]\n",
        "  y_val_dat = ys[train_obs:ys.shape[0],:]\n",
        "  stat_train = preds[0:train_obs,:]\n",
        "  stat_val = preds[train_obs:preds.shape[0],:]\n",
        "\n",
        "\n",
        "  if verbose == 1:\n",
        "      print(\"X DATA SHAPE:\", xs.shape)\n",
        "      print(\"X TRAINING DATA SHAPE:\", x_train_dat.shape)\n",
        "      print(\"STAT PRED DATA SHAPE:\", preds.shape)\n",
        "      print(\"X VALIDATION DATA SHAPE:\", x_val_dat.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # train-test split\n",
        "  if verbose == 1:\n",
        "\n",
        "    print(\"TRAIN SHAPE: \", x_train_dat.shape)\n",
        "    print(\"VAL SHAPE: \", x_val_dat.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # set up neural network for the given data\n",
        "\n",
        "  # Include stat preds into the data (will be for one type of CV and included in if-statement later)\n",
        "  x_train_dat = np.concatenate([x_train_dat, stat_train], axis = 1)\n",
        "  x_val_dat = np.concatenate([x_val_dat, stat_val], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # declare a sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  if nn_modeltype == \"feedforward\" and nn_task == \"regression\":\n",
        "\n",
        "    # create a simple linear feed forward model\n",
        "    model.add(layers.Dense(32, activation='relu',input_shape=[x_train_dat.shape[1],]))\n",
        "    # This layer is the recurent layer, which returns all previous data\n",
        "    model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "    # layer that reads the recurent layer\n",
        "    model.add(layers.Dense(y_train_dat.shape[1]))\n",
        "    model.compile(optimizer=RMSprop(learning_rate = 0.0001), loss='mse')\n",
        "\n",
        "  if nn_modeltype == \"feedforward\" and nn_task == \"classification\":\n",
        "    cce = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    # create a simple linear feed forward model\n",
        "    model.add(layers.Dense(32, activation='relu',input_shape=[x_train_dat.shape[1],]))\n",
        "    # This layer is the recurent layer, which returns all previous data\n",
        "    model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "    # layer that reads the recurent layer\n",
        "    model.add(layers.Dense(y_train_dat.shape[1], activation = \"softmax\"))\n",
        "    model.compile(optimizer=RMSprop(learning_rate = 0.0001), loss=cce)\n",
        "\n",
        "\n",
        "\n",
        "  # run NN model\n",
        "  test_one = model.fit(x = x_train_dat,\n",
        "                      y = y_train_dat,\n",
        "                      steps_per_epoch = x_train_dat.shape[0],\n",
        "                      epochs= epochs_in,\n",
        "                      batch_size = 1,\n",
        "                      validation_data= (x_val_dat, y_val_dat),\n",
        "                      callbacks=[callback],\n",
        "                      validation_steps= x_val_dat.shape[0],\n",
        "                      verbose = 1)\n",
        "  # print plot training graph\n",
        "  if verbose == 1:\n",
        "    history_plot(test_one)\n",
        "\n",
        "  # get NN predictions\n",
        "  nn_preds = model.predict(x_val_dat)\n",
        "\n",
        "  print(\"NN pred shape\", nn_preds.shape)\n",
        "  print(\"Y Shape Uncollapsed\", y_val_dat.shape )\n",
        "\n",
        "  # for classification tasks, turn optimal model prediction (categorical) back into regression prediction\n",
        "  if nn_task == \"classification\":\n",
        "    real_preds = np.zeros(shape = [nn_preds.shape[0],1])\n",
        "    nn_preds = nn_preds * stat_val\n",
        "    for row in range(0,nn_preds.shape[0]):\n",
        "\n",
        "      real_preds[row,0] = nn_preds[row, np.argmax(np.absolute(nn_preds[row,:]), axis=0)]\n",
        "\n",
        "    nn_preds = real_preds\n",
        "\n",
        "  print(\"Y Shape Reshapen\", nn_preds.shape)\n",
        "  print(nn_preds[0:5])\n",
        "  # get MSE for validation data\n",
        "  stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(test_window, y_val_dat, stat_val, nn_preds)\n",
        "\n",
        "\n",
        "  print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "  print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "  print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "  if nn_task == \"regression\":\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse\n",
        "  #if nn_task == \"classification\":\n"
      ],
      "metadata": {
        "id": "yQv9zLVxJHLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtEXVGeD5WYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_single_nn_wrapper2(\n",
        "    nn_modeltype,\n",
        "    nn_task,\n",
        "    train_split,\n",
        "    ticker,\n",
        "    model_names = ['auto_arima','complex_smoothing'],\n",
        "    predReturnFunctions = [new_aaPredFunction, new_cesPredFunction],\n",
        "    start_date_in = '2022-06-01',\n",
        "    end_date_in = '2023-09-30',\n",
        "    interval_in = '60m',\n",
        "    split = False,\n",
        "    train_size = 2000,\n",
        "    train_window = 10,\n",
        "    test_window = 1,\n",
        "    epochs_in = 200,\n",
        "    verbose = 1):\n",
        "\n",
        "  # use inputs to get stock data of the requested format\n",
        "  ticker_data = prep_ticker(ticker, intervals = interval_in, start_date = start_date_in, end_date = end_date_in, split = split)\n",
        "\n",
        "  # difference data to test if it makes a 'difference'\n",
        "  # update, differenced data seems to break auto CES\n",
        "  #ticker_data = ticker_data.diff()\n",
        "\n",
        "  # normalize data\n",
        "  ticker_mean = ticker_data.mean()\n",
        "  ticker_std = ticker_data.std()\n",
        "  ticker_data = (ticker_data - ticker_mean) / ticker_std\n",
        "\n",
        "  # break stock data into windows and return statistical model predictions\n",
        "  xs,ys,preds = stat_true_window_cross_val(ticker_data, model_names, predReturnFunctions, train_window, test_window)\n",
        "\n",
        "  # calculate train test split\n",
        "  train_obs = round(xs.shape[0] * train_split)\n",
        "\n",
        "  if nn_task == \"regression\":\n",
        "\n",
        "    # divide matrices\n",
        "    x_train_dat = xs[0:train_obs,:]\n",
        "    x_val_dat = xs[train_obs:xs.shape[0],:]\n",
        "    y_train_dat = ys[0:train_obs,:]\n",
        "    y_val_dat = ys[train_obs:ys.shape[0],:]\n",
        "    stat_train = preds[0:train_obs,:]\n",
        "    stat_val = preds[train_obs:preds.shape[0],:]\n",
        "\n",
        "  if nn_task == \"classification\":\n",
        "\n",
        "    # generate categorical outcome data (which model was optimal) if task is classification\n",
        "    # only works with 1-ahead windows for now\n",
        "    stat_model_number = len(model_names)\n",
        "    y_optimals = np.zeros(shape = [ys.shape[0], stat_model_number])\n",
        "\n",
        "    #iterate through the occurences to find optimal model in the ensemble in each\n",
        "    errors = np.zeros(shape = preds.shape)\n",
        "    for i in range(0, y_optimals.shape[0]):\n",
        "      yval = ys[i,0]\n",
        "      #print(\"error shape: \", errors.shape)\n",
        "      for j in range(0, errors.shape[1]):\n",
        "        errors[i,j] = yval - preds[i,j]\n",
        "\n",
        "      y_optimals[i, np.argmin(errors[i,:], axis=0)] = 1\n",
        "\n",
        "    # divide matrices, saving both cateogrical and regression info\n",
        "    x_train_dat = xs[0:train_obs,:]\n",
        "    x_val_dat = xs[train_obs:xs.shape[0],:]\n",
        "    y_train_dat = y_optimals[0:train_obs,:]\n",
        "    y_train_reg_real = ys[0:train_obs,:]\n",
        "    y_val_dat = y_optimals[train_obs:ys.shape[0],:]\n",
        "    y_val_reg_real = ys[train_obs:ys.shape[0],:]\n",
        "    stat_train = preds[0:train_obs,:]\n",
        "    stat_val = preds[train_obs:preds.shape[0],:]\n",
        "\n",
        "\n",
        "  \"\"\"    print(\"val optimals: \")\n",
        "    print(y_val_dat[0:5,:])\n",
        "    print(\"val real\")\n",
        "    print(y_val_reg_real[0:5,:])\n",
        "    print(\"stat val\")\n",
        "    print(stat_val[0:5,:])\n",
        "    print(\"errors\")\n",
        "    print(errors[0:5,:])\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Train obs: \", x_train_dat.shape[0], \"Val Obs:\", x_val_dat.shape[0])\n",
        "\n",
        "  # train-test split\n",
        "  if verbose == 1:\n",
        "\n",
        "    print(\"TRAIN SHAPE: \", x_train_dat.shape)\n",
        "    print(\"VAL SHAPE: \", x_val_dat.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # set up neural network for the given data\n",
        "\n",
        "  # Include stat preds into the data (will be for one type of CV and included in if-statement later)\n",
        "  x_train_dat = np.concatenate([x_train_dat, stat_train], axis = 1)\n",
        "  x_val_dat = np.concatenate([x_val_dat, stat_val], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # declare a sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  if nn_modeltype == \"feedforward\" and nn_task == \"regression\":\n",
        "\n",
        "    # create a simple linear feed forward model\n",
        "    model.add(layers.Dense(32, activation='relu',input_shape=[x_train_dat.shape[1],]))\n",
        "    # This layer is the recurent layer, which returns all previous data\n",
        "    model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "    # layer that reads the recurent layer\n",
        "    model.add(layers.Dense(y_train_dat.shape[1]))\n",
        "    model.compile(optimizer=RMSprop(learning_rate = 0.0001), loss='mse')\n",
        "\n",
        "  if nn_modeltype == \"feedforward\" and nn_task == \"classification\":\n",
        "    cce = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    # create a simple linear feed forward model\n",
        "    model.add(layers.Dense(32, activation='relu',input_shape=[x_train_dat.shape[1],]))\n",
        "    # This layer is the recurent layer, which returns all previous data\n",
        "    model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "    # layer that reads the recurent layer\n",
        "    model.add(layers.Dense(y_train_dat.shape[1], activation = \"softmax\"))\n",
        "    model.compile(optimizer=RMSprop(learning_rate = 0.0001), loss=cce)\n",
        "\n",
        "\n",
        "\n",
        "  # run NN model\n",
        "  test_one = model.fit(x = x_train_dat,\n",
        "                      y = y_train_dat,\n",
        "                      steps_per_epoch = x_train_dat.shape[0],\n",
        "                      epochs= epochs_in,\n",
        "                      batch_size = 1,\n",
        "                      validation_data= (x_val_dat, y_val_dat),\n",
        "                      callbacks=[callback],\n",
        "                      validation_steps= x_val_dat.shape[0],\n",
        "                      verbose = 1)\n",
        "  # print plot training graph\n",
        "  if verbose == 1:\n",
        "    history_plot(test_one)\n",
        "\n",
        "  # get NN predictions\n",
        "  nn_preds = model.predict(x_val_dat)\n",
        "\n",
        "  print(\"NN pred shape\", nn_preds.shape)\n",
        "  print(\"Y Shape Uncollapsed\", y_val_dat.shape )\n",
        "\n",
        "\n",
        "  if nn_task == \"regression\":\n",
        "\n",
        "    stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(test_window, y_val_dat, stat_val, nn_preds)\n",
        "\n",
        "\n",
        "    print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "    print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "    print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse\n",
        "\n",
        "  # for classification tasks, turn optimal model prediction (categorical) back into regression prediction\n",
        "  if nn_task == \"classification\":\n",
        "    print(\"unedited preds\")\n",
        "    print(nn_preds[0:5,])\n",
        "\n",
        "    real_preds = np.zeros(shape = [nn_preds.shape[0],1])\n",
        "    for row in range(0,nn_preds.shape[0]):\n",
        "\n",
        "      real_preds[row,0] = stat_val[row, np.argmax(np.absolute(nn_preds[row,:]), axis=0)]\n",
        "\n",
        "    nn_preds = real_preds\n",
        "\n",
        "\n",
        "    print(\"Y Shape Reshapen\", nn_preds.shape)\n",
        "    print(nn_preds[0:5])\n",
        "    print(\"Stat Preds\")\n",
        "    print(stat_val[0:5,:])\n",
        "    print(\"Real Ys\")\n",
        "    print(y_val_reg_real[0:5])\n",
        "\n",
        "    stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(test_window, y_val_reg_real, stat_val, nn_preds)\n",
        "\n",
        "\n",
        "    print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "    print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "    print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse"
      ],
      "metadata": {
        "id": "v-vBqycK5Wmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New DL Examples\n"
      ],
      "metadata": {
        "id": "Klo_b31iOH6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.matrix([[0,2],[-2,0],[0,5]])\n",
        "\n",
        "a[np.argmax(a, axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nrCHzCVvXhc",
        "outputId": "fb7e3b9b-448d-4c31-97ff-b7390bf7567d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[[-2,  0]],\n",
              "\n",
              "        [[-2,  0]],\n",
              "\n",
              "        [[-2,  0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "L1=0\n",
        "L2=1e-3\n",
        "callback = keras.callbacks.EarlyStopping(monitor='loss',patience=20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "full_single_nn_wrapper2(\n",
        "    \"feedforward\",\n",
        "    \"classification\",\n",
        "    0.8,\n",
        "    \"SPY\",\n",
        "    model_names = ['auto_arima','complex_smoothing'],\n",
        "    predReturnFunctions = [new_aaPredFunction, new_cesPredFunction],\n",
        "    start_date_in = '2023-12-25',\n",
        "    end_date_in = '2024-02-15',\n",
        "    interval_in = '5m',\n",
        "    split = False,\n",
        "    train_size = 2000,\n",
        "    train_window = 30,\n",
        "    test_window = 1,\n",
        "    verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WQHbj-ELcAbA",
        "outputId": "b77b695d-5c8d-4ef5-c6a2-56cca8ff81c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(88, 33)\n",
            "(2730,)\n",
            "Train obs:  70 Val Obs: 18\n",
            "TRAIN SHAPE:  (70, 30)\n",
            "VAL SHAPE:  (18, 30)\n",
            "Epoch 1/200\n",
            "70/70 [==============================] - 1s 5ms/step - loss: 0.7770 - val_loss: 0.9211\n",
            "Epoch 2/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7644 - val_loss: 0.8443\n",
            "Epoch 3/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7556 - val_loss: 0.7790\n",
            "Epoch 4/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7522 - val_loss: 0.7668\n",
            "Epoch 5/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7496 - val_loss: 0.7488\n",
            "Epoch 6/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7476 - val_loss: 0.7443\n",
            "Epoch 7/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7448 - val_loss: 0.7395\n",
            "Epoch 8/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7429 - val_loss: 0.7371\n",
            "Epoch 9/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7421 - val_loss: 0.7301\n",
            "Epoch 10/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7396 - val_loss: 0.7273\n",
            "Epoch 11/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.7373 - val_loss: 0.7240\n",
            "Epoch 12/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.7349 - val_loss: 0.7230\n",
            "Epoch 13/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.7326 - val_loss: 0.7191\n",
            "Epoch 14/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 0.7201\n",
            "Epoch 15/200\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.7280 - val_loss: 0.7229\n",
            "Epoch 16/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7258 - val_loss: 0.7184\n",
            "Epoch 17/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7253 - val_loss: 0.7159\n",
            "Epoch 18/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7227 - val_loss: 0.7132\n",
            "Epoch 19/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7195 - val_loss: 0.7197\n",
            "Epoch 20/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7188 - val_loss: 0.7216\n",
            "Epoch 21/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7173 - val_loss: 0.7146\n",
            "Epoch 22/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7135 - val_loss: 0.7046\n",
            "Epoch 23/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7129 - val_loss: 0.7008\n",
            "Epoch 24/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7126 - val_loss: 0.7021\n",
            "Epoch 25/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7098 - val_loss: 0.7039\n",
            "Epoch 26/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.7071 - val_loss: 0.7045\n",
            "Epoch 27/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7064 - val_loss: 0.6989\n",
            "Epoch 28/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7057 - val_loss: 0.6937\n",
            "Epoch 29/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.6907\n",
            "Epoch 30/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.6882\n",
            "Epoch 31/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.6850\n",
            "Epoch 32/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6979 - val_loss: 0.6859\n",
            "Epoch 33/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.6848\n",
            "Epoch 34/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.6819\n",
            "Epoch 35/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6833\n",
            "Epoch 36/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6904 - val_loss: 0.6859\n",
            "Epoch 37/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6884 - val_loss: 0.6798\n",
            "Epoch 38/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6867 - val_loss: 0.6800\n",
            "Epoch 39/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6851 - val_loss: 0.6802\n",
            "Epoch 40/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6815 - val_loss: 0.6697\n",
            "Epoch 41/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6828 - val_loss: 0.6688\n",
            "Epoch 42/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6798 - val_loss: 0.6690\n",
            "Epoch 43/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6787 - val_loss: 0.6752\n",
            "Epoch 44/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6783 - val_loss: 0.6728\n",
            "Epoch 45/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6755 - val_loss: 0.6754\n",
            "Epoch 46/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6741 - val_loss: 0.6730\n",
            "Epoch 47/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6714 - val_loss: 0.6642\n",
            "Epoch 48/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6691 - val_loss: 0.6594\n",
            "Epoch 49/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6686 - val_loss: 0.6597\n",
            "Epoch 50/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6664 - val_loss: 0.6590\n",
            "Epoch 51/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6645 - val_loss: 0.6604\n",
            "Epoch 52/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6626 - val_loss: 0.6636\n",
            "Epoch 53/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6606 - val_loss: 0.6731\n",
            "Epoch 54/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6592 - val_loss: 0.6662\n",
            "Epoch 55/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6573 - val_loss: 0.6652\n",
            "Epoch 56/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6531 - val_loss: 0.6584\n",
            "Epoch 57/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6528 - val_loss: 0.6580\n",
            "Epoch 58/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 0.6634\n",
            "Epoch 59/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6493 - val_loss: 0.6618\n",
            "Epoch 60/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.6494 - val_loss: 0.6515\n",
            "Epoch 61/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6475 - val_loss: 0.6640\n",
            "Epoch 62/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6463 - val_loss: 0.6513\n",
            "Epoch 63/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6452 - val_loss: 0.6474\n",
            "Epoch 64/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6420 - val_loss: 0.6576\n",
            "Epoch 65/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6385 - val_loss: 0.6472\n",
            "Epoch 66/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6378 - val_loss: 0.6496\n",
            "Epoch 67/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6396 - val_loss: 0.6375\n",
            "Epoch 68/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 0.6389\n",
            "Epoch 69/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6357 - val_loss: 0.6348\n",
            "Epoch 70/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6309 - val_loss: 0.6410\n",
            "Epoch 71/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6298 - val_loss: 0.6512\n",
            "Epoch 72/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6296 - val_loss: 0.6468\n",
            "Epoch 73/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6265 - val_loss: 0.6462\n",
            "Epoch 74/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6223 - val_loss: 0.6390\n",
            "Epoch 75/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 0.6387\n",
            "Epoch 76/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6181 - val_loss: 0.6389\n",
            "Epoch 77/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6207 - val_loss: 0.6508\n",
            "Epoch 78/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6166 - val_loss: 0.6514\n",
            "Epoch 79/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6116 - val_loss: 0.6423\n",
            "Epoch 80/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6149 - val_loss: 0.6322\n",
            "Epoch 81/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6126 - val_loss: 0.6334\n",
            "Epoch 82/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6111 - val_loss: 0.6272\n",
            "Epoch 83/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6061 - val_loss: 0.6323\n",
            "Epoch 84/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6074 - val_loss: 0.6314\n",
            "Epoch 85/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6059 - val_loss: 0.6316\n",
            "Epoch 86/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6054 - val_loss: 0.6686\n",
            "Epoch 87/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.6022 - val_loss: 0.6482\n",
            "Epoch 88/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6009 - val_loss: 0.6422\n",
            "Epoch 89/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5963 - val_loss: 0.6287\n",
            "Epoch 90/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5988 - val_loss: 0.6354\n",
            "Epoch 91/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5972 - val_loss: 0.6320\n",
            "Epoch 92/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5923 - val_loss: 0.6191\n",
            "Epoch 93/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5939 - val_loss: 0.6248\n",
            "Epoch 94/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5888 - val_loss: 0.6234\n",
            "Epoch 95/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5910 - val_loss: 0.6216\n",
            "Epoch 96/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5869 - val_loss: 0.6283\n",
            "Epoch 97/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5898 - val_loss: 0.6342\n",
            "Epoch 98/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5857 - val_loss: 0.6409\n",
            "Epoch 99/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5831 - val_loss: 0.6526\n",
            "Epoch 100/200\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.5794 - val_loss: 0.6858\n",
            "Epoch 101/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5798 - val_loss: 0.6610\n",
            "Epoch 102/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5806 - val_loss: 0.6539\n",
            "Epoch 103/200\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.5772 - val_loss: 0.6412\n",
            "Epoch 104/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.5744 - val_loss: 0.6270\n",
            "Epoch 105/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5768 - val_loss: 0.6219\n",
            "Epoch 106/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5758 - val_loss: 0.6274\n",
            "Epoch 107/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5734 - val_loss: 0.6226\n",
            "Epoch 108/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5740 - val_loss: 0.6240\n",
            "Epoch 109/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5715 - val_loss: 0.6286\n",
            "Epoch 110/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5685 - val_loss: 0.6189\n",
            "Epoch 111/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5690 - val_loss: 0.6294\n",
            "Epoch 112/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5665 - val_loss: 0.6253\n",
            "Epoch 113/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5668 - val_loss: 0.6255\n",
            "Epoch 114/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5629 - val_loss: 0.6294\n",
            "Epoch 115/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5618 - val_loss: 0.6355\n",
            "Epoch 116/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5619 - val_loss: 0.6206\n",
            "Epoch 117/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5580 - val_loss: 0.6143\n",
            "Epoch 118/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5595 - val_loss: 0.6139\n",
            "Epoch 119/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5587 - val_loss: 0.6229\n",
            "Epoch 120/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5544 - val_loss: 0.6714\n",
            "Epoch 121/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5580 - val_loss: 0.6598\n",
            "Epoch 122/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5536 - val_loss: 0.6542\n",
            "Epoch 123/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5560 - val_loss: 0.6437\n",
            "Epoch 124/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5510 - val_loss: 0.6420\n",
            "Epoch 125/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5532 - val_loss: 0.6349\n",
            "Epoch 126/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5538 - val_loss: 0.6447\n",
            "Epoch 127/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5470 - val_loss: 0.6491\n",
            "Epoch 128/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5470 - val_loss: 0.6147\n",
            "Epoch 129/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5471 - val_loss: 0.6092\n",
            "Epoch 130/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5481 - val_loss: 0.6121\n",
            "Epoch 131/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5457 - val_loss: 0.5977\n",
            "Epoch 132/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5468 - val_loss: 0.5990\n",
            "Epoch 133/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5443 - val_loss: 0.6313\n",
            "Epoch 134/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5411 - val_loss: 0.6328\n",
            "Epoch 135/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5435 - val_loss: 0.6575\n",
            "Epoch 136/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5425 - val_loss: 0.6418\n",
            "Epoch 137/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5373 - val_loss: 0.6395\n",
            "Epoch 138/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5424 - val_loss: 0.6291\n",
            "Epoch 139/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5384 - val_loss: 0.6306\n",
            "Epoch 140/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5364 - val_loss: 0.6009\n",
            "Epoch 141/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5358 - val_loss: 0.6188\n",
            "Epoch 142/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5313 - val_loss: 0.6387\n",
            "Epoch 143/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5370 - val_loss: 0.6611\n",
            "Epoch 144/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.5329 - val_loss: 0.6238\n",
            "Epoch 145/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5353 - val_loss: 0.6316\n",
            "Epoch 146/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5318 - val_loss: 0.6460\n",
            "Epoch 147/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5257 - val_loss: 0.6863\n",
            "Epoch 148/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.5282 - val_loss: 0.6489\n",
            "Epoch 149/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5290 - val_loss: 0.6036\n",
            "Epoch 150/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5294 - val_loss: 0.5943\n",
            "Epoch 151/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5301 - val_loss: 0.6027\n",
            "Epoch 152/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5271 - val_loss: 0.6146\n",
            "Epoch 153/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5254 - val_loss: 0.6067\n",
            "Epoch 154/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5344 - val_loss: 0.5938\n",
            "Epoch 155/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5260 - val_loss: 0.5990\n",
            "Epoch 156/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5227 - val_loss: 0.5831\n",
            "Epoch 157/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5244 - val_loss: 0.6169\n",
            "Epoch 158/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5231 - val_loss: 0.6361\n",
            "Epoch 159/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5231 - val_loss: 0.6132\n",
            "Epoch 160/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5227 - val_loss: 0.6228\n",
            "Epoch 161/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5164 - val_loss: 0.6180\n",
            "Epoch 162/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5224 - val_loss: 0.6174\n",
            "Epoch 163/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5169 - val_loss: 0.6355\n",
            "Epoch 164/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5204 - val_loss: 0.6325\n",
            "Epoch 165/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5172 - val_loss: 0.6054\n",
            "Epoch 166/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5173 - val_loss: 0.6210\n",
            "Epoch 167/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5239 - val_loss: 0.6411\n",
            "Epoch 168/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5174 - val_loss: 0.6677\n",
            "Epoch 169/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5161 - val_loss: 0.6485\n",
            "Epoch 170/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5150 - val_loss: 0.6504\n",
            "Epoch 171/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5146 - val_loss: 0.6572\n",
            "Epoch 172/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5160 - val_loss: 0.6365\n",
            "Epoch 173/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5147 - val_loss: 0.6128\n",
            "Epoch 174/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5106 - val_loss: 0.6277\n",
            "Epoch 175/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5135 - val_loss: 0.6475\n",
            "Epoch 176/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5135 - val_loss: 0.6505\n",
            "Epoch 177/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5134 - val_loss: 0.6392\n",
            "Epoch 178/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5112 - val_loss: 0.6689\n",
            "Epoch 179/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5105 - val_loss: 0.6647\n",
            "Epoch 180/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5151 - val_loss: 0.7021\n",
            "Epoch 181/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5104 - val_loss: 0.7607\n",
            "Epoch 182/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5078 - val_loss: 0.6979\n",
            "Epoch 183/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5026 - val_loss: 0.6209\n",
            "Epoch 184/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5055 - val_loss: 0.6645\n",
            "Epoch 185/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5075 - val_loss: 0.6385\n",
            "Epoch 186/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5090 - val_loss: 0.6696\n",
            "Epoch 187/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5008 - val_loss: 0.6530\n",
            "Epoch 188/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5033 - val_loss: 0.6886\n",
            "Epoch 189/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5109 - val_loss: 0.7104\n",
            "Epoch 190/200\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.5065 - val_loss: 0.6469\n",
            "Epoch 191/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5066 - val_loss: 0.7040\n",
            "Epoch 192/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5081 - val_loss: 0.7273\n",
            "Epoch 193/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5030 - val_loss: 0.7355\n",
            "Epoch 194/200\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.5073 - val_loss: 0.6593\n",
            "Epoch 195/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5045 - val_loss: 0.7128\n",
            "Epoch 196/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5051 - val_loss: 0.6852\n",
            "Epoch 197/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5019 - val_loss: 0.6741\n",
            "Epoch 198/200\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5030 - val_loss: 0.6898\n",
            "Epoch 199/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5025 - val_loss: 0.7061\n",
            "Epoch 200/200\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4989 - val_loss: 0.6227\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "NN pred shape (18, 2)\n",
            "Y Shape Uncollapsed (18, 2)\n",
            "unedited preds\n",
            "[[0.6581637  0.34183627]\n",
            " [0.609825   0.390175  ]\n",
            " [0.51418144 0.4858186 ]\n",
            " [0.22321111 0.7767889 ]\n",
            " [0.49081305 0.5091869 ]]\n",
            "Y Shape Reshapen (18, 1)\n",
            "[[0.9698909 ]\n",
            " [0.95859582]\n",
            " [1.09986988]\n",
            " [1.47522891]\n",
            " [1.50663388]]\n",
            "Stat Preds\n",
            "[[0.9698909  0.96861601]\n",
            " [0.95859582 0.95913672]\n",
            " [1.09986988 1.0886178 ]\n",
            " [1.47013061 1.47522891]\n",
            " [1.50722212 1.50663388]]\n",
            "Real Ys\n",
            "[[0.99294918]\n",
            " [0.99595652]\n",
            " [1.31173911]\n",
            " [1.47865089]\n",
            " [1.50822558]]\n",
            "2\n",
            "STAT VAL ENSEMBLE MSE:  0.0033541734922553756\n",
            "NN VAL MODEL MSE:  0.003243907338741355\n",
            "NN+STAT VAL ENSEMBLE MSE:  0.003313954979316786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0033541734922553756, 0.003243907338741355, 0.003313954979316786)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"full_test = prep_ticker('QQQ', intervals = \"5m\", start_date = '2023-12-15', end_date = '2024-01-30', split = False)\n",
        "\n",
        "print(np.mean(full_test, axis = 0))\n",
        "\n",
        "full_test = (full_test - np.mean(full_test, axis = 0) ) / np.std(full_test ,axis = 0)\n",
        "\n",
        "print(full_test.head)\"\"\"\n",
        "\n",
        "full_test = prep_ticker('SPY', intervals = \"5m\", start_date = '2023-12-25', end_date = '2024-01-30', split = False)\n",
        "\n",
        "full_test = full_test.diff()\n",
        "print(np.mean(full_test, axis = 0))\n",
        "\n",
        "full_test = (full_test - np.mean(full_test, axis = 0) ) / np.std(full_test ,axis = 0)\n",
        "\n",
        "print(full_test.head)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRoxdMf4XcQz",
        "outputId": "1dbc1b26-5adb-4c9a-c152-282793d09446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Close    0.009308\n",
            "dtype: float64\n",
            "<bound method NDFrame.head of                               Close\n",
            "Datetime                           \n",
            "2023-12-26 09:30:00-05:00       NaN\n",
            "2023-12-26 09:35:00-05:00 -0.060254\n",
            "2023-12-26 09:40:00-05:00  0.158100\n",
            "2023-12-26 09:45:00-05:00 -0.060254\n",
            "2023-12-26 09:50:00-05:00 -0.606044\n",
            "...                             ...\n",
            "2024-01-29 15:35:00-05:00 -0.715221\n",
            "2024-01-29 15:40:00-05:00 -0.496962\n",
            "2024-01-29 15:45:00-05:00 -0.450036\n",
            "2024-01-29 15:50:00-05:00  0.251667\n",
            "2024-01-29 15:55:00-05:00  1.935964\n",
            "\n",
            "[1794 rows x 1 columns]>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = stat_true_window_cross_val(full_test, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction], train_window= 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY12vWSUo8qW",
        "outputId": "32b3cfb1-6569-4b2d-a532-ba1e76f5b436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(107, 23)\n",
            "(2262,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##sanity_check = np.array([1,2,3,4,5,6])\n",
        "\n",
        "#xs,ys,preds = stat_true_window_cross_val(sanity_check, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction], train_window= 5, test_window = 1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqHxcneKR2cq",
        "outputId": "796d3a62-7def-4613-83d8-779e8ee67c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(6,)\n",
            "[[1.         2.         3.         4.         5.         6.\n",
            "  6.         3.77109623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "first_try_x = output[0:80,np.r_[0:10, 11:13]]\n",
        "first_try_y = output[0:80, 10:11]\n",
        "\n",
        "first_try_x_val = output[80:107,np.r_[0:10, 11:13]]\n",
        "first_try_y_val = output[80:107, 10:11]\n",
        "\n",
        "print(first_try_x.shape)\n",
        "print(first_try_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bin6j0ElqL59",
        "outputId": "cee21e63-e075-48f5-d963-b8504bf57313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 12)\n",
            "(80, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"L1=0\n",
        "L2=1e-3\n",
        "\n",
        "# create a sequential model once again\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(32, activation='relu',input_shape=[12]))\n",
        "# This layer is the recurent layer, which returns all previous data\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "\n",
        "\n",
        "# layer that reads the recurent layer\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "\n",
        "test_one = model.fit(x = first_try_x, y = first_try_y, steps_per_epoch=150, epochs=20, batch_size = 1, validation_data= (first_try_x_val, first_try_y_val), validation_steps=60, verbose = 1)\n",
        "\"\"\"\n",
        "\n",
        "print(len(first_try_x))\n",
        "\n",
        "# Hyper parameters\n",
        "L1=0\n",
        "L2=1e-3\n",
        "callback = keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
        "\n",
        "\n",
        "# create a sequential model once again\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(32, activation='relu',input_shape=[12,]))\n",
        "# This layer is the recurent layer, which returns all previous data\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "#model.add(layers.Dropout(rate=0.25))\n",
        "#model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "#model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "\n",
        "\n",
        "# layer that reads the recurent layer\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(learning_rate = 0.00001), loss='mse')\n",
        "\n",
        "\n",
        "test_one = model.fit(x = first_try_x, y = first_try_y, steps_per_epoch = 80, epochs=150, batch_size = 1, validation_data= (first_try_x_val, first_try_y_val),callbacks=[callback], validation_steps=25, verbose = 1)\n"
      ],
      "metadata": {
        "id": "yLvdMoURncTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf616d9c-a671-4370-9bbb-db897257d52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n",
            "Epoch 1/150\n",
            "80/80 [==============================] - 2s 12ms/step - loss: 1.2630 - val_loss: 1.1436\n",
            "Epoch 2/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 1.1412 - val_loss: 1.1157\n",
            "Epoch 3/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 1.0294 - val_loss: 1.0837\n",
            "Epoch 4/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.9234 - val_loss: 1.0533\n",
            "Epoch 5/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.8274 - val_loss: 1.0219\n",
            "Epoch 6/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7413 - val_loss: 0.9864\n",
            "Epoch 7/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.6530 - val_loss: 0.9519\n",
            "Epoch 8/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.5724 - val_loss: 0.9161\n",
            "Epoch 9/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5048 - val_loss: 0.8792\n",
            "Epoch 10/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4381 - val_loss: 0.8468\n",
            "Epoch 11/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3804 - val_loss: 0.8121\n",
            "Epoch 12/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.7740\n",
            "Epoch 13/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 0.7383\n",
            "Epoch 14/150\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.7030\n",
            "Epoch 15/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.1980 - val_loss: 0.6666\n",
            "Epoch 16/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1669 - val_loss: 0.6322\n",
            "Epoch 17/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1390 - val_loss: 0.5942\n",
            "Epoch 18/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1175 - val_loss: 0.5619\n",
            "Epoch 19/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 0.5232\n",
            "Epoch 20/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.4863\n",
            "Epoch 21/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0764 - val_loss: 0.4501\n",
            "Epoch 22/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.4117\n",
            "Epoch 23/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0659 - val_loss: 0.3756\n",
            "Epoch 24/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.3417\n",
            "Epoch 25/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.3080\n",
            "Epoch 26/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.2762\n",
            "Epoch 27/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.2461\n",
            "Epoch 28/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.2181\n",
            "Epoch 29/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.1955\n",
            "Epoch 30/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.1723\n",
            "Epoch 31/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.1521\n",
            "Epoch 32/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.1346\n",
            "Epoch 33/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.1188\n",
            "Epoch 34/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.1038\n",
            "Epoch 35/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.0909\n",
            "Epoch 36/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.0803\n",
            "Epoch 37/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0548 - val_loss: 0.0712\n",
            "Epoch 38/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0543 - val_loss: 0.0633\n",
            "Epoch 39/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0538 - val_loss: 0.0571\n",
            "Epoch 40/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0534 - val_loss: 0.0520\n",
            "Epoch 41/150\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.0529 - val_loss: 0.0485\n",
            "Epoch 42/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.0459\n",
            "Epoch 43/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0447\n",
            "Epoch 44/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0443\n",
            "Epoch 45/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.0449\n",
            "Epoch 46/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.0464\n",
            "Epoch 47/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.0484\n",
            "Epoch 48/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.0506\n",
            "Epoch 49/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0499 - val_loss: 0.0534\n",
            "Epoch 50/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0495 - val_loss: 0.0565\n",
            "Epoch 51/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.0602\n",
            "Epoch 52/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.0633\n",
            "Epoch 53/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0669\n",
            "Epoch 54/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0483 - val_loss: 0.0690\n",
            "Epoch 55/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0716\n",
            "Epoch 56/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.0741\n",
            "Epoch 57/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.0757\n",
            "Epoch 58/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.0770\n",
            "Epoch 59/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0469 - val_loss: 0.0781\n",
            "Epoch 60/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.0794\n",
            "Epoch 61/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0804\n",
            "Epoch 62/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.0801\n",
            "Epoch 63/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.0799\n",
            "Epoch 64/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.0795\n",
            "Epoch 65/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.0792\n",
            "Epoch 66/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.0777\n",
            "Epoch 67/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.0772\n",
            "Epoch 68/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0757\n",
            "Epoch 69/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0753\n",
            "Epoch 70/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0734\n",
            "Epoch 71/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0723\n",
            "Epoch 72/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.0710\n",
            "Epoch 73/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.0695\n",
            "Epoch 74/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0676\n",
            "Epoch 75/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0662\n",
            "Epoch 76/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0641\n",
            "Epoch 77/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.0623\n",
            "Epoch 78/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0428 - val_loss: 0.0603\n",
            "Epoch 79/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0426 - val_loss: 0.0586\n",
            "Epoch 80/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.0570\n",
            "Epoch 81/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0422 - val_loss: 0.0557\n",
            "Epoch 82/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0420 - val_loss: 0.0542\n",
            "Epoch 83/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0531\n",
            "Epoch 84/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.0515\n",
            "Epoch 85/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.0503\n",
            "Epoch 86/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.0488\n",
            "Epoch 87/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0475\n",
            "Epoch 88/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0459\n",
            "Epoch 89/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0446\n",
            "Epoch 90/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0437\n",
            "Epoch 91/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.0430\n",
            "Epoch 92/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.0418\n",
            "Epoch 93/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0410\n",
            "Epoch 94/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0403\n",
            "Epoch 95/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0398\n",
            "Epoch 96/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0394\n",
            "Epoch 97/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0389\n",
            "Epoch 98/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0383\n",
            "Epoch 99/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0379\n",
            "Epoch 100/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0375\n",
            "Epoch 101/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0372\n",
            "Epoch 102/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.0369\n",
            "Epoch 103/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0366\n",
            "Epoch 104/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0384 - val_loss: 0.0364\n",
            "Epoch 105/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0362\n",
            "Epoch 106/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.0360\n",
            "Epoch 107/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0359\n",
            "Epoch 108/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0358\n",
            "Epoch 109/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0357\n",
            "Epoch 110/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0356\n",
            "Epoch 111/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0355\n",
            "Epoch 112/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0356\n",
            "Epoch 113/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0356\n",
            "Epoch 114/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0356\n",
            "Epoch 115/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0357\n",
            "Epoch 116/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0357\n",
            "Epoch 117/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.0358\n",
            "Epoch 118/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0359\n",
            "Epoch 119/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0360\n",
            "Epoch 120/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0362 - val_loss: 0.0361\n",
            "Epoch 121/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0361 - val_loss: 0.0362\n",
            "Epoch 122/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 0.0363\n",
            "Epoch 123/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0364\n",
            "Epoch 124/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0357 - val_loss: 0.0365\n",
            "Epoch 125/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0367\n",
            "Epoch 126/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0369\n",
            "Epoch 127/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0368\n",
            "Epoch 128/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0370\n",
            "Epoch 129/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0370\n",
            "Epoch 130/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0372\n",
            "Epoch 131/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0373\n",
            "Epoch 132/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0375\n",
            "Epoch 133/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0375\n",
            "Epoch 134/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0376\n",
            "Epoch 135/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0379\n",
            "Epoch 136/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0376\n",
            "Epoch 137/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0379\n",
            "Epoch 138/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0377\n",
            "Epoch 139/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0376\n",
            "Epoch 140/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0377\n",
            "Epoch 141/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0375\n",
            "Epoch 142/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0376\n",
            "Epoch 143/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0378\n",
            "Epoch 144/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0376\n",
            "Epoch 145/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0376\n",
            "Epoch 146/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0375\n",
            "Epoch 147/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0374\n",
            "Epoch 148/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0375\n",
            "Epoch 149/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0328 - val_loss: 0.0375\n",
            "Epoch 150/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_plot(test_one)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "BreXjQMHucxP",
        "outputId": "e9d0a27b-bf6a-4169-fe7a-8903aba1a966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.predict(first_try_x))\n",
        "\n",
        "nn_preds = model.predict(first_try_x)\n",
        "stat_preds = first_try_x[:,10:12]\n",
        "print(stat_preds[0:3,:])\n",
        "print(first_try_x[0:3,:])\n",
        "real_ys = first_try_y\n",
        "window = 1\n",
        "\n",
        "stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(window, real_ys, stat_preds, nn_preds)\n",
        "\n",
        "print(\"STAT TRAIN ENSEMBLE MSE: \", stat_avg_mse)\n",
        "print(\"NN TRAIN MODEL MSE: \", nn_mse)\n",
        "print(\"NN+STAT TRAIN ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "\n",
        "nn_preds2 = model.predict(first_try_x_val)\n",
        "stat_preds2 = first_try_x_val[:,10:12]\n",
        "print(stat_preds2[0:3,:])\n",
        "real_ys2 = first_try_y_val\n",
        "window2 = 1\n",
        "\n",
        "stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(window, real_ys2, stat_preds2, nn_preds2)\n",
        "\n",
        "print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N1QzdPB5Xyf",
        "outputId": "2b6980c8-5f36-43e6-dd69-123b4ef4f1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 15ms/step\n",
            "[[-1.02839118 -1.00976611]\n",
            " [-1.02246525 -1.00636658]\n",
            " [-1.26201821 -1.25186096]]\n",
            "[[-1.18837558 -1.17737103 -1.121501   -1.17059781 -1.10056137 -1.18244965\n",
            "  -1.15197791 -1.10795973 -1.08849253 -1.09441329 -1.02839118 -1.00976611]\n",
            " [-1.01821843 -1.00215076 -0.98268356 -0.98776218 -1.01653933 -1.02669658\n",
            "  -1.04955297 -1.05378429 -1.02502265 -1.06055752 -1.02246525 -1.00636658]\n",
            " [-1.13081612 -1.13165825 -1.1214855  -1.13335285 -1.09780248 -1.07409879\n",
            "  -1.07071476 -1.14556118 -1.26370764 -1.20784277 -1.26201821 -1.25186096]]\n",
            "2\n",
            "STAT TRAIN ENSEMBLE MSE:  0.002580036183811661\n",
            "NN TRAIN MODEL MSE:  0.0034495044041024624\n",
            "NN+STAT TRAIN ENSEMBLE MSE:  0.0020458753035366454\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[[-0.36473955 -0.30719559]\n",
            " [ 0.03647174  0.04324496]\n",
            " [ 0.36326026  0.37759717]]\n",
            "2\n",
            "STAT VAL ENSEMBLE MSE:  0.002839849369788067\n",
            "NN VAL MODEL MSE:  0.008564990617821129\n",
            "NN+STAT VAL ENSEMBLE MSE:  0.0020829388950082527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data2 = cv_data[1:120]\n",
        "\n",
        "cv_data2[0:12] = np.array([0,1,2,3,4,5,6,7,8,9,10,11]).reshape([12,1])\n",
        "output = stat_true_window_cross_val(cv_data2, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "HFHezSU2AQPb",
        "outputId": "6d44c730-96f5-4fd5-dd61-522af7816b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cv_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c366178482a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv_data2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat_true_window_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_aaPredFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cesPredFunction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0:3,0:13])\n",
        "print(cv_data[0:12])"
      ],
      "metadata": {
        "id": "CMN8rUW_EsM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testResultsQQQ, trainSeriesQQQ, testSeriesQQQ = prep_ticker('QQQ')\n",
        "\n",
        "print(testResultsQQQ.head() )\n",
        "print(trainSeriesQQQ.head() )\n",
        "print(testSeriesQQQ.head() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjvXcXzlkHvq",
        "outputId": "13b0e978-c764-4595-8710-4efa008f77f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price  auto_arima  dyn_theta  auto_ets  complex_smoothing\n",
            "0  376.239990           0          0         0                  0\n",
            "1  377.880005           0          0         0                  0\n",
            "2  378.390015           0          0         0                  0\n",
            "3  378.799988           0          0         0                  0\n",
            "4  379.730011           0          0         0                  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testResultsQQQ, trainSeriesQQQ, testSeriesQQQ = prep_ticker('QQQ', model_list = ['auto_arima', 'dyn_theta', 'complex_smoothing'])\n",
        "\n"
      ],
      "metadata": {
        "id": "sLT6_TwVmgc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "3dba41b7-f16b-48c2-cc10-faf439cf784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'yf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-34105c547167>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestResultsQQQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSeriesQQQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSeriesQQQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QQQ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dyn_theta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-69f3cddda564>\u001b[0m in \u001b[0;36mprep_ticker\u001b[0;34m(ticker, start_date, end_date, intervals, train_size, model_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprep_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2022-06-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2023-09-30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintervals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'60m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dyn_theta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auto_ets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m#print(data.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qqqpreds, qqqerrors = stat_test_wrapper(trainSeriesQQQ, testSeriesQQQ, testResultsQQQ, 200, 10)\n"
      ],
      "metadata": {
        "id": "5g9CjbLhshfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}