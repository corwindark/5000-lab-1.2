{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "l14ED15pN7TW",
        "N3FbBbIBN-db"
      ],
      "authorship_tag": "ABX9TyPCmCmBXIxwoddlLXRIPgjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corwindark/5000-lab-1.2/blob/main/dl_ens_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Setup - Run Every Time"
      ],
      "metadata": {
        "id": "IcQ0TfR_NvtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GmcHy4PefJh",
        "outputId": "55a62047-1bbb-4aa7-a05c-efd08b2e0071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TENSORFLOW VERSION: 2.15.0\n",
            "PYTORCH VERSION: 2.1.0+cu121\n",
            "KERAS VERSION: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "# TENSORFLOW\n",
        "import tensorflow as tf\n",
        "print(\"TENSORFLOW VERSION:\",tf.__version__)\n",
        "\n",
        "# PYTORCH\n",
        "import torch\n",
        "print(\"PYTORCH VERSION:\",torch.__version__)\n",
        "\n",
        "# KERAS\n",
        "import keras;\n",
        "print(\"KERAS VERSION:\",keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgLd74Zteqck",
        "outputId": "c5c94971-0cb4-4f5e-8270-1fa077c58831"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET SYSTEM INFO\n",
        "# code modified from: https://stackoverflow.com/questions/110362/how-can-i-find-the-current-os-in-python\n",
        "\n",
        "import platform\n",
        "import multiprocessing\n",
        "import sys\n",
        "import psutil\n",
        "\n",
        "def linux_distribution():\n",
        "  try:\n",
        "    return platform.linux_distribution()\n",
        "  except:\n",
        "    return \"N/A\"\n",
        "\n",
        "def dist():\n",
        "  try:\n",
        "    return platform.dist()\n",
        "  except:\n",
        "    return \"N/A\"\n",
        "\n",
        "num_cores=multiprocessing.cpu_count()\n",
        "\n",
        "print(\"\"\"\n",
        "Python version: %s\n",
        "dist: %s\n",
        "num_cores: %s\n",
        "linux_distribution: %s\n",
        "system: %s\n",
        "machine: %s\n",
        "platform: %s\n",
        "uname: %s\n",
        "version: %s\n",
        "RAM: %s\n",
        "\"\"\" % (\n",
        "sys.version.split('\\n'),\n",
        "str(dist()),\n",
        "num_cores,\n",
        "linux_distribution(),\n",
        "platform.system(),\n",
        "platform.machine(),\n",
        "platform.platform(),\n",
        "platform.uname(),\n",
        "platform.version(),\n",
        "psutil.virtual_memory().total*10**(-9.)\n",
        "))\n",
        "\n",
        "# print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWIeaqgue6WW",
        "outputId": "8e113b2d-a2d9-435f-ccd4-734c9a5bf1ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Python version: ['3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]']\n",
            "dist: N/A\n",
            "num_cores: 2\n",
            "linux_distribution: N/A\n",
            "system: Linux\n",
            "machine: x86_64\n",
            "platform: Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "uname: uname_result(system='Linux', node='dfd263d6121c', release='6.1.58+', version='#1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023', machine='x86_64')\n",
            "version: #1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\n",
            "RAM: 13.60945152\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()\n",
        "torch.cuda.get_device_properties(0).total_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "hHpcauGQfDuU",
        "outputId": "7c720733-1eb7-476e-ac37-19c6076f7cea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d923db55dd9d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ],
      "metadata": {
        "id": "-YU0DRxuga_4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsforecast\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKOyzoq0kuAB",
        "outputId": "a4b3cbc3-26d7-4a12-ae4a-a18a86678f25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import AutoCES, AutoARIMA, AutoETS, DynamicOptimizedTheta\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI2srljshLUI",
        "outputId": "16b505e3-3483-4199-cc46-45126d12d491"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Stat Functions"
      ],
      "metadata": {
        "id": "l14ED15pN7TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crossValidation(train, test, window, outputFrame, modelName, predReturnFunction):\n",
        "\n",
        "    totalPredictions = pd.Series(dtype = 'float64')\n",
        "    windows = len(test) // window\n",
        "\n",
        "    predictionIndex = 0\n",
        "\n",
        "    for i in range(0, windows):\n",
        "        # debug\n",
        "        #print(\"Window: \", i, \"Method: \", modelName)\n",
        "\n",
        "        # How many observations to move forward each frame\n",
        "        addNum = i * window\n",
        "        # Combine training data with additional test window\n",
        "        intermediateData = pd.concat([train, test[:addNum]])\n",
        "        # Generate prediction of the given window size\n",
        "        prediction = predReturnFunction(intermediateData, window)\n",
        "        #print(prediction)\n",
        "        # Check if we have multiple predictions in the window\n",
        "        if len(prediction) > 1:\n",
        "            # Store each predicted value with a loop\n",
        "            for j in range(0,window):\n",
        "\n",
        "                # Store in the prediction-comparison frame\n",
        "                outputFrame[modelName][predictionIndex] = prediction[j]\n",
        "\n",
        "                # Move to next open spot\n",
        "                predictionIndex += 1\n",
        "        else:\n",
        "\n",
        "            outputFrame[modelName][predictionIndex] = prediction[0]\n",
        "            predictionIndex += 1\n"
      ],
      "metadata": {
        "id": "HVpcUSSjhJOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stat_test_wrapper(seriesTrain, seriesTest, testDF,  test_out_size = 100, window_size = 1):\n",
        "    # Set the number of observations to be included in the test set\n",
        "\n",
        "  testDFCopy = testDF.copy()\n",
        "\n",
        "  tempTest = seriesTest[:test_out_size]\n",
        "\n",
        "  # Auto Arima\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'auto_arima', aaPredFunction)\n",
        "\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'complex_smoothing', cesPredFunction)\n",
        "\n",
        "  #crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'auto_ets', etsPredFunction)\n",
        "\n",
        "  crossValidation(seriesTrain, tempTest, window_size, testDFCopy, 'dyn_theta', dotPredFunction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plot2 = testDFCopy.iloc[0:test_out_size,:]\n",
        "\n",
        "  preds = plot2.loc[:,['auto_arima', 'complex_smoothing', 'dyn_theta']]\n",
        "  # ensemble methods\n",
        "  plot2.loc[:,'mean_ens'] = preds.mean(axis = 1)\n",
        "  plot2.loc[:,'median_ens'] = preds.median(axis = 1)\n",
        "\n",
        "\n",
        "  plot2.loc[:,'auto_arima'] = abs( plot2.loc[:,'auto_arima'] - plot2.loc[:,'Price'] )\n",
        "  # auto ets is not very good apparently\n",
        "  #plot2.loc[:,'auto_ets'] = abs( plot2.loc[:,'auto_ets'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'complex_smoothing'] = abs( plot2.loc[:,'complex_smoothing'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'dyn_theta'] = abs( plot2.loc[:,'dyn_theta'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'mean_ens'] = abs( plot2.loc[:,'mean_ens'] - plot2.loc[:,'Price'] )\n",
        "  plot2.loc[:,'median_ens'] = abs( plot2.loc[:,'median_ens'] - plot2.loc[:,'Price'] )\n",
        "\n",
        "\n",
        "\n",
        "  # get the index as a column for plotting\n",
        "  plot2 = plot2.reset_index()\n",
        "\n",
        "  errorDat = plot2.copy()\n",
        "\n",
        "  names = ['auto_arima', 'dyn_theta', 'complex_smoothing']\n",
        "  errorDat['optimal'] = errorDat[names].idxmin(axis=\"columns\")\n",
        "\n",
        "  #print(\"reached loop\")\n",
        "  \"\"\"\n",
        "  for index in range(0, errorDat.shape[0]):\n",
        "      #print(index)\n",
        "\n",
        "      predictionErrors = errorDat.loc[index,['auto_arima', 'auto_ets', 'dyn_theta', 'complex_smoothing'] ]\n",
        "\n",
        "      #print(predictionErrors)\n",
        "\n",
        "      #print(min(abs(predictionErrors)))\n",
        "      #errorDat.loc[index ,'optimal'] = min(abs(predictionErrors))\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  return testDFCopy, errorDat"
      ],
      "metadata": {
        "id": "e0eVuMiIlN01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aaPredFunction(dataIn, windowSize):\n",
        "    model = AutoARIMA()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    prediction = fit1.predict(h = windowSize)\n",
        "    return prediction.get('mean')\n",
        "\n",
        "\n",
        "def cesPredFunction(dataIn, windowSize):\n",
        "    model = AutoCES()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def etsPredFunction(dataIn, windowSize):\n",
        "    model = AutoETS()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def dotPredFunction(dataIn, windowSize):\n",
        "    model = DynamicOptimizedTheta()\n",
        "    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "\n",
        "\n",
        "# Set the number of observations to be included in the test set\n",
        "test_out_size = 200\n",
        "\n",
        "tempTest = seriesTest[:test_out_size]\n"
      ],
      "metadata": {
        "id": "tM9QbHylhSvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prep_ticker(ticker, start_date = '2022-06-01', end_date = '2023-09-30', intervals = '60m', split = True, train_size = 2000, model_list = ['auto_arima', 'dyn_theta', 'auto_ets', 'complex_smoothing']):\n",
        "  data = yf.download(ticker,start_date, end_date, interval = intervals)\n",
        "  #print(data.head())\n",
        "  data.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "  data_train = data[:train_size]\n",
        "  data_test = data[train_size:]\n",
        "\n",
        "  test_size = len(data_test)\n",
        "\n",
        "\n",
        "  data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "  data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "  seriesTrain = data_train.set_index('Datetime')\n",
        "  seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "  testResultsDF = pd.DataFrame(index = range(test_size))\n",
        "  testResultsDF['Price'] = 0\n",
        "\n",
        "  #print(data.head())\n",
        "\n",
        "  # Get a clean format for close prices\n",
        "  for i in range(0,test_size):\n",
        "      testResultsDF['Price'][i] = seriesTest.values[i][0]\n",
        "\n",
        "\n",
        "  # Initialize empty cells for the statistical forecasts\n",
        "  for modeltype in model_list:\n",
        "    testResultsDF[modeltype] = 0\n",
        "\n",
        "  return testResultsDF, seriesTrain, seriesTest"
      ],
      "metadata": {
        "id": "GRRsuTtaWZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Stat Examples\n"
      ],
      "metadata": {
        "id": "N3FbBbIBN-db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = yf.download('SPY','2022-06-01','2023-09-30', interval =\"60m\")\n",
        "%matplotlib inline\n",
        "\n",
        "data['Adj Close'].plot()\n",
        "plt.ion()\n",
        "print(data)\n",
        "\n",
        "data.reset_index(inplace = True)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "yzKoe4nigzre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 2000\n",
        "\n",
        "data_train = data[:train_size]\n",
        "data_test = data[train_size:]\n",
        "\n",
        "test_size = len(data_test)\n",
        "\n",
        "\n",
        "data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "seriesTrain = data_train.set_index('Datetime')\n",
        "seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "cv_data = data[['Close']]"
      ],
      "metadata": {
        "id": "Xo_WQK4WhEky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reformat the test data into a dataframe we can add our predictions to\n",
        "\n",
        "testResultsDF = pd.DataFrame(index = range(test_size))\n",
        "testResultsDF['Price'] = 0\n",
        "# Get a clean format for close prices\n",
        "for i in range(0,test_size):\n",
        "    testResultsDF['Price'][i] = seriesTest.values[i][0]\n",
        "\n",
        "\n",
        "# Initialize empty cells for the statistical forecasts\n",
        "testResultsDF['auto_arima'] = 0\n",
        "testResultsDF['dyn_theta'] = 0\n",
        "#testResultsDF['auto_ets'] = 0\n",
        "testResultsDF['complex_smoothing'] = 0"
      ],
      "metadata": {
        "id": "nAYAKdiChGmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Auto Arima\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_arima', aaPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'complex_smoothing', cesPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_ets', etsPredFunction)\n",
        "\n",
        "crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'dyn_theta', dotPredFunction)\n"
      ],
      "metadata": {
        "id": "nzAn1r3eAO3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testResultsDF)\n",
        "\n",
        "plot1 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot1 = plot1.reset_index()\n",
        "\n",
        "plot1 = pd.melt(plot1, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta', 'Price'])\n",
        "\n",
        "print(plot1)\n",
        "\n",
        "sns.lineplot(plot1, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "3pR4l_YPhdoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot1 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot1 = plot1.reset_index()\n",
        "\n",
        "plot1 = pd.melt(plot1, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta', 'Price'])\n",
        "\n",
        "print(plot1)\n",
        "\n",
        "sns.lineplot(plot1, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "99el4haXjZJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testResultsDF)\n",
        "\n",
        "plot2 = testResultsDF.iloc[0:test_out_size,:]\n",
        "\n",
        "plot2['auto_arima'] = plot2['auto_arima'] - plot2['Price']\n",
        "plot2['auto_ets'] = plot2['auto_ets'] - plot2['Price']\n",
        "plot2['complex_smoothing'] = plot2['complex_smoothing'] - plot2['Price']\n",
        "plot2['dyn_theta'] = plot2['dyn_theta'] - plot2['Price']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# get the index as a column for plotting\n",
        "plot2 = plot2.reset_index()\n",
        "\n",
        "errorDat = plot2.copy()\n",
        "\n",
        "plot2 = pd.melt(plot2, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta'])\n",
        "\n",
        "sns.barplot(plot2, x = 'index', y = 'value', hue = 'variable')"
      ],
      "metadata": {
        "id": "P-p9QbHCj5gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(AAPLerrors.head())\n",
        "print(AAPLerrors['optimal'].value_counts() )\n",
        "\n",
        "print(AAPLerrors['auto_arima'].mean())\n",
        "print(AAPLerrors['dyn_theta'].mean())\n",
        "print(AAPLerrors['complex_smoothing'].mean())\n",
        "print(AAPLerrors['mean_ens'].mean())\n",
        "\n",
        "optimals = ['auto_arima', 'dyn_theta', 'complex_smoothing']\n",
        "print(AAPLerrors[optimals].min(axis=\"columns\").mean())\n"
      ],
      "metadata": {
        "id": "aeAJVba9zC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AAPLpreds, AAPLerrors = stat_test_wrapper(trainSeriesAAPL, testSeriesAAPL, testResultsAAPL, 25, 1)\n"
      ],
      "metadata": {
        "id": "G57EBqz8y2S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errorDat['optimal'] = 0\n",
        "\n",
        "for index in range(0, errorDat.shape[0]):\n",
        "   # print(index)\n",
        "    predictionErrors = errorDat.loc[:,('auto_arima', 'auto_ets', 'dyn_theta', 'complex_smoothing') ]\n",
        "\n",
        "    #print(predictionErrors.min())\n",
        "\n",
        "    errorDat.loc[index ,'optimal'] = min(predictionErrors)\n",
        "\n",
        "errorDat"
      ],
      "metadata": {
        "id": "rRxuc2fUly-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qqqerrors.head())\n",
        "print(qqqerrors['optimal'].value_counts() )\n",
        "# QQQ, 22-6-1 to 23-9-30, 1hr,\n",
        "# > 5-1 is auto_arima 4, complex smoothing 1\n",
        "# > 100-1 is auto_arima 50, complex smoothing 44, dyn_theta 6\n",
        "# > 200-20 is auto_arima 41, complex_smoothing 111, dyn_theta 48\n",
        "# > 200-50 is auto_arima 18, c_s 92, dyn_theta 90\n",
        "# > 200-100 is a_a 60, c_s 78, d_t 62\n",
        "# AAPL, 22-6-1 to 23-9-30, 1hr,\n",
        "# 100 - 1 is a_a 42, c_s 51, d_t 7\n",
        "\n"
      ],
      "metadata": {
        "id": "5oF603FGsIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# New DL Functions"
      ],
      "metadata": {
        "id": "Cx2aXYcaOA0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting more different financial datasets\n",
        "\n",
        "\n",
        "def prep_ticker(ticker, start_date = '2022-06-01', end_date = '2023-09-30', intervals = '60m', split = True, train_size = 2000):\n",
        "  data = yf.download(ticker,start_date, end_date, interval = intervals)\n",
        "\n",
        "  #print(data.head)\n",
        "\n",
        "  data.reset_index(inplace = True)\n",
        "\n",
        "  if split == True:\n",
        "    data_train = data[:train_size]\n",
        "    data_test = data[train_size:]\n",
        "\n",
        "    test_size = len(data_test)\n",
        "\n",
        "\n",
        "    data_test = data_test[[\"Datetime\", \"Close\"]]\n",
        "    data_train = data_train[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "    seriesTrain = data_train.set_index('Datetime')\n",
        "    seriesTest = data_test.set_index('Datetime')\n",
        "\n",
        "\n",
        "    return seriesTrain, seriesTest\n",
        "\n",
        "  elif split == False:\n",
        "    data_out = data\n",
        "\n",
        "    data_out = data_out[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "    seriesOut = data_out.set_index('Datetime')\n",
        "\n",
        "\n",
        "    return seriesOut\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZVIFG9onTH7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_aaPredFunction(dataIn, windowSize):\n",
        "    model = AutoARIMA()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    prediction = fit1.predict(h = windowSize)\n",
        "    return prediction.get('mean')\n",
        "\n",
        "\n",
        "def new_cesPredFunction(dataIn, windowSize):\n",
        "    model = AutoCES()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def new_etsPredFunction(dataIn, windowSize):\n",
        "    model = AutoETS()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')\n",
        "\n",
        "def new_dotPredFunction(dataIn, windowSize):\n",
        "    model = DynamicOptimizedTheta()\n",
        "    fit1 = model.fit(dataIn)\n",
        "    predictionList = fit1.predict(windowSize)\n",
        "    return predictionList.get('mean')"
      ],
      "metadata": {
        "id": "nzhZY8XEPjkZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stat_true_window_cross_val(unsplitdata, model_names, predReturnFunctions, train_window = 10, test_window = 1):\n",
        "    totalPredictions = pd.Series(dtype = 'float64')\n",
        "    windows = len(unsplitdata) // (train_window + test_window)\n",
        "    columns = train_window + (test_window * (1 + len(model_names)))\n",
        "\n",
        "    pred_df = np.zeros([windows, columns])\n",
        "    print(pred_df.shape)\n",
        "\n",
        "    unsplitdata = np.array(unsplitdata)\n",
        "    unsplitdata = unsplitdata.reshape((unsplitdata.shape[0],))\n",
        "    print(unsplitdata.shape)\n",
        "\n",
        "    # indexes loop through the data\n",
        "    predictionIndex = 0\n",
        "    startindex = 0\n",
        "\n",
        "    for i in range(0, windows):\n",
        "        # debug\n",
        "        #print(\"Window: \", i)\n",
        "\n",
        "        train_window_dat = unsplitdata[startindex:(startindex+train_window)]\n",
        "        #print(train_window_dat)\n",
        "        test_window_dat = unsplitdata[(startindex+train_window ): (test_window + startindex+train_window)]\n",
        "\n",
        "        startindex = startindex + train_window + test_window\n",
        "\n",
        "        #print(pred_df[i,0:train_window])\n",
        "        #print(pred_df[i,0:train_window].shape)\n",
        "\n",
        "        #print(train_window_dat.shape)\n",
        "        #print( train_window_dat.reshape((train_window,)).shape)\n",
        "\n",
        "        pred_df[i,0:train_window] = np.reshape(train_window_dat, [train_window])\n",
        "        pred_df[i,(train_window):(train_window + test_window)] = test_window_dat\n",
        "\n",
        "        for modnum, modelfunction in enumerate(predReturnFunctions):\n",
        "          # account for how many columns in output array over to shift for this model\n",
        "          # add 1 to fit the real values\n",
        "          #print(modnum)\n",
        "          offset = ((modnum + 1) * test_window) + train_window\n",
        "\n",
        "          # Generate prediction of the given window size\n",
        "          prediction = modelfunction(train_window_dat, test_window)\n",
        "          #print(prediction[0])\n",
        "\n",
        "          # Check if we have multiple predictions in the window\n",
        "          if len(prediction) > 1:\n",
        "              # Store each predicted value with a loop\n",
        "              for j in range(0,test_window):\n",
        "\n",
        "                  # Store in the prediction-comparison frame\n",
        "                  pred_df[i,j + offset] = prediction[j]\n",
        "\n",
        "          else:\n",
        "\n",
        "              pred_df[i,offset] = prediction[0]\n",
        "\n",
        "    # currently configured to split up data into 3 matrices instead of returning together\n",
        "    x_windows = pred_df[:,0:train_window]\n",
        "    real_ys = pred_df[:,train_window:(train_window + test_window)]\n",
        "    stat_preds = pred_df[:,(train_window + test_window):(columns+1) ]\n",
        "\n",
        "\n",
        "    #print(pred_df)\n",
        "\n",
        "    return x_windows, real_ys, stat_preds\n"
      ],
      "metadata": {
        "id": "skBWEjboSjvV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def history_plot(history):\n",
        "    FS=18   #FONT SIZE\n",
        "    # PLOTTING THE TRAINING AND VALIDATION LOSS\n",
        "    history_dict = history.history\n",
        "    loss_values = history_dict[\"loss\"]\n",
        "    val_loss_values = history_dict[\"val_loss\"]\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "    plt.title(\"Training and validation loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ql8LVy1foPmW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(window_size, real_data, stat_pred, nn_pred):\n",
        "    # declare values we will calculate when looping through the real data\n",
        "    stat_avg_mse, nn_mse, nn_avg_mse = 0,0,0\n",
        "    # find out how many stat models are in the ensemble\n",
        "    num_stat_mods = int(stat_pred.shape[1] / window_size)\n",
        "\n",
        "    print(num_stat_mods)\n",
        "\n",
        "    for row in range(0, real_data.shape[0]):\n",
        "      for obs in range(0,real_data.shape[1]):\n",
        "\n",
        "        # real value\n",
        "        rv = real_data[row,obs]\n",
        "\n",
        "        # squared error for neural network model\n",
        "        nn_mse += (rv - nn_pred[row,obs]) ** 2\n",
        "\n",
        "        # numerator for stat model avg\n",
        "        stat_numerator = 0\n",
        "\n",
        "        for mod in range(0,num_stat_mods):\n",
        "          # add each models prediction for the given datapoint\n",
        "          stat_numerator += stat_pred[row,obs + (mod*window_size)]\n",
        "\n",
        "        # numerator for the stat+NN ensemble\n",
        "        nn_stat_numerator = stat_numerator + nn_pred[row,obs]\n",
        "\n",
        "        # find the ensemble prediction for the datapoint\n",
        "        stat_avg = stat_numerator / num_stat_mods\n",
        "        nn_stat_avg = nn_stat_numerator / (num_stat_mods + 1)\n",
        "\n",
        "        # calculate the squared error for these ensembles\n",
        "        stat_avg_mse += (rv - stat_avg) ** 2\n",
        "        nn_avg_mse += (rv - nn_stat_avg) ** 2\n",
        "\n",
        "    # find mean square error instead of total\n",
        "    n_obs = real_data.shape[0]\n",
        "    nn_mse = nn_mse / n_obs\n",
        "    stat_avg_mse = stat_avg_mse / n_obs\n",
        "    nn_avg_mse = nn_avg_mse / n_obs\n",
        "\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse\n",
        "\n"
      ],
      "metadata": {
        "id": "VtAPlWf4owQw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_single_nn_wrapper(\n",
        "    nn_modeltype,\n",
        "    nn_task,\n",
        "    train_split,\n",
        "    ticker,\n",
        "    model_names = ['auto_arima','complex_smoothing'],\n",
        "    predReturnFunctions = [new_aaPredFunction, new_cesPredFunction],\n",
        "    start_date_in = '2022-06-01',\n",
        "    end_date_in = '2023-09-30',\n",
        "    interval_in = '60m',\n",
        "    split = False,\n",
        "    train_size = 2000,\n",
        "    train_window = 10,\n",
        "    test_window = 1,\n",
        "    epochs_in = 50,\n",
        "    verbose = 1):\n",
        "\n",
        "  # use inputs to get stock data of the requested format\n",
        "  ticker_data = prep_ticker(ticker, intervals = interval_in, start_date = start_date_in, end_date = end_date_in, split = split)\n",
        "\n",
        "  # normalize data\n",
        "  ticker_mean = ticker_data.mean()\n",
        "  ticker_std = ticker_data.std()\n",
        "  ticker_data = (ticker_data - ticker_mean) / ticker_std\n",
        "\n",
        "  # break stock data into windows and return statistical model predictions\n",
        "  xs,ys,preds = stat_true_window_cross_val(ticker_data, model_names, predReturnFunctions, train_window, test_window)\n",
        "\n",
        "  # status update\n",
        "  if verbose == 1:\n",
        "    print(\"STAT MODELS TRAINED\")\n",
        "\n",
        "  # calculate train test split\n",
        "  train_obs = round(xs.shape[0] * train_split)\n",
        "\n",
        "\n",
        "\n",
        "  # divide matrices\n",
        "  x_train_dat = xs[0:train_obs,:]\n",
        "  x_val_dat = xs[train_obs:xs.shape[0],:]\n",
        "  y_train_dat = ys[0:train_obs,:]\n",
        "  y_val_dat = ys[train_obs:ys.shape[0],:]\n",
        "  stat_train = preds[0:train_obs,:]\n",
        "  stat_val = preds[train_obs:preds.shape[0],:]\n",
        "\n",
        "\n",
        "  if verbose == 1:\n",
        "      print(\"X DATA SHAPE:\", xs.shape)\n",
        "      print(\"X TRAINING DATA SHAPE:\", x_train_dat.shape)\n",
        "      print(\"STAT PRED DATA SHAPE:\", preds.shape)\n",
        "      print(\"X VALIDATION DATA SHAPE:\", x_val_dat.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # train-test split\n",
        "  if verbose == 1:\n",
        "\n",
        "    print(\"TRAIN SHAPE: \", x_train_dat.shape)\n",
        "    print(\"VAL SHAPE: \", x_val_dat.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # set up neural network for the given data\n",
        "\n",
        "  # Include stat preds into the data (will be for one type of CV and included in if-statement later)\n",
        "  x_train_dat = np.concatenate([x_train_dat, stat_train], axis = 1)\n",
        "  x_val_dat = np.concatenate([x_val_dat, stat_val], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # declare a sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  if nn_modeltype == \"feedforward\" and nn_task == \"regression\":\n",
        "\n",
        "    # create a simple linear feed forward model\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu',input_shape=[x_train_dat.shape[1],]))\n",
        "    # This layer is the recurent layer, which returns all previous data\n",
        "    model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "\n",
        "    # layer that reads the recurent layer\n",
        "    model.add(layers.Dense(y_train_dat.shape[1]))\n",
        "    model.compile(optimizer=RMSprop(learning_rate = 0.0001), loss='mse')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # run NN model\n",
        "  test_one = model.fit(x = x_train_dat,\n",
        "                      y = y_train_dat,\n",
        "                      steps_per_epoch = x_train_dat.shape[0],\n",
        "                      epochs= epochs_in,\n",
        "                      batch_size = 1,\n",
        "                      validation_data= (x_val_dat, y_val_dat),\n",
        "                      callbacks=[callback],\n",
        "                      validation_steps= x_val_dat.shape[0],\n",
        "                      verbose = 1)\n",
        "  # print plot training graph\n",
        "  if verbose == 1:\n",
        "    history_plot(test_one)\n",
        "\n",
        "  # get NN predictions\n",
        "  nn_preds = model.predict(x_val_dat)\n",
        "\n",
        "  print(\"NN pred shape\", nn_preds.shape)\n",
        "  print(\"Y Shape\", y_val_dat.shape )\n",
        "\n",
        "  # get MSE for validation data\n",
        "  stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(test_window, y_val_dat, stat_val, nn_preds)\n",
        "\n",
        "  print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "  print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "  print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "  if nn_task == \"regression\":\n",
        "    return stat_avg_mse, nn_mse, nn_avg_mse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQv9zLVxJHLp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New DL Examples\n"
      ],
      "metadata": {
        "id": "Klo_b31iOH6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "L1=0\n",
        "L2=1e-3\n",
        "callback = keras.callbacks.EarlyStopping(monitor='loss',patience=20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "full_single_nn_wrapper(\n",
        "    \"feedforward\",\n",
        "    \"regression\",\n",
        "    0.8,\n",
        "    \"SPY\",\n",
        "    model_names = ['auto_arima','complex_smoothing'],\n",
        "    predReturnFunctions = [new_aaPredFunction, new_cesPredFunction],\n",
        "    start_date_in = '2023-12-15',\n",
        "    end_date_in = '2024-01-30',\n",
        "    interval_in = '5m',\n",
        "    split = False,\n",
        "    train_size = 2000,\n",
        "    train_window = 20,\n",
        "    test_window = 5,\n",
        "    verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQHbj-ELcAbA",
        "outputId": "97b8388b-717f-4127-9dba-fc7c5444f06a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 35)\n",
            "(2262,)\n",
            "STAT MODELS TRAINED\n",
            "X DATA SHAPE: (90, 20)\n",
            "X TRAINING DATA SHAPE: (72, 20)\n",
            "STAT PRED DATA SHAPE: (90, 10)\n",
            "X VALIDATION DATA SHAPE: (18, 20)\n",
            "TRAIN SHAPE:  (72, 20)\n",
            "VAL SHAPE:  (18, 20)\n",
            "Epoch 1/50\n",
            "72/72 [==============================] - 1s 3ms/step - loss: 0.4394 - val_loss: 3.6269\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.2187 - val_loss: 3.0966\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1232 - val_loss: 2.3225\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 1.7915\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 1.3234\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0866 - val_loss: 0.8691\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0815 - val_loss: 0.6477\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0753 - val_loss: 0.3705\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.2293\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.1354\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.0996\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0983\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.1205\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0588 - val_loss: 0.1311\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.1408\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.1381\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.1642\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0539 - val_loss: 0.1504\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.1526\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0518 - val_loss: 0.1568\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.1621\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - 0s 1ms/step - loss: 0.0500 - val_loss: 0.1431\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.1531\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0479 - val_loss: 0.1383\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.1347\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.1300\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.1351\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - 0s 1ms/step - loss: 0.0449 - val_loss: 0.1250\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.1148\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.1268\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.1210\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.1066\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.1099\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.1101\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.1001\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.0868\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0979\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0888\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.1034\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.1095\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0960\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.1019\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - 0s 1ms/step - loss: 0.0372 - val_loss: 0.0968\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0837\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0941\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0916\n",
            "Epoch 47/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0985\n",
            "Epoch 48/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0887\n",
            "Epoch 49/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0914\n",
            "Epoch 50/50\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0868\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "NN pred shape (18, 5)\n",
            "Y Shape (18, 5)\n",
            "2\n",
            "STAT VAL ENSEMBLE MSE:  0.011116362793365807\n",
            "NN VAL MODEL MSE:  0.3154049763774058\n",
            "NN+STAT VAL ENSEMBLE MSE:  0.04064959612965088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.011116362793365807, 0.3154049763774058, 0.04064959612965088)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"full_test = prep_ticker('QQQ', intervals = \"5m\", start_date = '2023-12-15', end_date = '2024-01-30', split = False)\n",
        "\n",
        "print(np.mean(full_test, axis = 0))\n",
        "\n",
        "full_test = (full_test - np.mean(full_test, axis = 0) ) / np.std(full_test ,axis = 0)\n",
        "\n",
        "print(full_test.head)\"\"\"\n",
        "\n",
        "full_test = prep_ticker('SPY', intervals = \"5m\", start_date = '2023-12-15', end_date = '2024-01-30', split = False)\n",
        "\n",
        "print(np.mean(full_test, axis = 0))\n",
        "\n",
        "full_test = (full_test - np.mean(full_test, axis = 0) ) / np.std(full_test ,axis = 0)\n",
        "\n",
        "print(full_test.head)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRoxdMf4XcQz",
        "outputId": "98369099-3625-4b47-be4e-635b0997a34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Close    476.134572\n",
            "dtype: float64\n",
            "<bound method NDFrame.head of                               Close\n",
            "Datetime                           \n",
            "2023-12-15 09:30:00-05:00 -1.188376\n",
            "2023-12-15 09:35:00-05:00 -1.177371\n",
            "2023-12-15 09:40:00-05:00 -1.121501\n",
            "2023-12-15 09:45:00-05:00 -1.170598\n",
            "2023-12-15 09:50:00-05:00 -1.100561\n",
            "...                             ...\n",
            "2024-01-29 15:35:00-05:00  2.487009\n",
            "2024-01-29 15:40:00-05:00  2.461610\n",
            "2024-01-29 15:45:00-05:00  2.438759\n",
            "2024-01-29 15:50:00-05:00  2.453995\n",
            "2024-01-29 15:55:00-05:00  2.560651\n",
            "\n",
            "[2262 rows x 1 columns]>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = stat_true_window_cross_val(full_test, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction], train_window= 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY12vWSUo8qW",
        "outputId": "32b3cfb1-6569-4b2d-a532-ba1e76f5b436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(107, 23)\n",
            "(2262,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##sanity_check = np.array([1,2,3,4,5,6])\n",
        "\n",
        "#xs,ys,preds = stat_true_window_cross_val(sanity_check, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction], train_window= 5, test_window = 1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqHxcneKR2cq",
        "outputId": "796d3a62-7def-4613-83d8-779e8ee67c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(6,)\n",
            "[[1.         2.         3.         4.         5.         6.\n",
            "  6.         3.77109623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "first_try_x = output[0:80,np.r_[0:10, 11:13]]\n",
        "first_try_y = output[0:80, 10:11]\n",
        "\n",
        "first_try_x_val = output[80:107,np.r_[0:10, 11:13]]\n",
        "first_try_y_val = output[80:107, 10:11]\n",
        "\n",
        "print(first_try_x.shape)\n",
        "print(first_try_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bin6j0ElqL59",
        "outputId": "cee21e63-e075-48f5-d963-b8504bf57313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 12)\n",
            "(80, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"L1=0\n",
        "L2=1e-3\n",
        "\n",
        "# create a sequential model once again\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(32, activation='relu',input_shape=[12]))\n",
        "# This layer is the recurent layer, which returns all previous data\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "\n",
        "\n",
        "# layer that reads the recurent layer\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "\n",
        "test_one = model.fit(x = first_try_x, y = first_try_y, steps_per_epoch=150, epochs=20, batch_size = 1, validation_data= (first_try_x_val, first_try_y_val), validation_steps=60, verbose = 1)\n",
        "\"\"\"\n",
        "\n",
        "print(len(first_try_x))\n",
        "\n",
        "# Hyper parameters\n",
        "L1=0\n",
        "L2=1e-3\n",
        "callback = keras.callbacks.EarlyStopping(monitor='loss',patience=5)\n",
        "\n",
        "\n",
        "# create a sequential model once again\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(32, activation='relu',input_shape=[12,]))\n",
        "# This layer is the recurent layer, which returns all previous data\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "#model.add(layers.Dropout(rate=0.25))\n",
        "#model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "#model.add(layers.Dense(64, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n",
        "\n",
        "\n",
        "# layer that reads the recurent layer\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(learning_rate = 0.00001), loss='mse')\n",
        "\n",
        "\n",
        "test_one = model.fit(x = first_try_x, y = first_try_y, steps_per_epoch = 80, epochs=150, batch_size = 1, validation_data= (first_try_x_val, first_try_y_val),callbacks=[callback], validation_steps=25, verbose = 1)\n"
      ],
      "metadata": {
        "id": "yLvdMoURncTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf616d9c-a671-4370-9bbb-db897257d52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n",
            "Epoch 1/150\n",
            "80/80 [==============================] - 2s 12ms/step - loss: 1.2630 - val_loss: 1.1436\n",
            "Epoch 2/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 1.1412 - val_loss: 1.1157\n",
            "Epoch 3/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 1.0294 - val_loss: 1.0837\n",
            "Epoch 4/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.9234 - val_loss: 1.0533\n",
            "Epoch 5/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.8274 - val_loss: 1.0219\n",
            "Epoch 6/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7413 - val_loss: 0.9864\n",
            "Epoch 7/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.6530 - val_loss: 0.9519\n",
            "Epoch 8/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.5724 - val_loss: 0.9161\n",
            "Epoch 9/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5048 - val_loss: 0.8792\n",
            "Epoch 10/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.4381 - val_loss: 0.8468\n",
            "Epoch 11/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3804 - val_loss: 0.8121\n",
            "Epoch 12/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.7740\n",
            "Epoch 13/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 0.7383\n",
            "Epoch 14/150\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.7030\n",
            "Epoch 15/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.1980 - val_loss: 0.6666\n",
            "Epoch 16/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1669 - val_loss: 0.6322\n",
            "Epoch 17/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1390 - val_loss: 0.5942\n",
            "Epoch 18/150\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1175 - val_loss: 0.5619\n",
            "Epoch 19/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 0.5232\n",
            "Epoch 20/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.4863\n",
            "Epoch 21/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0764 - val_loss: 0.4501\n",
            "Epoch 22/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.4117\n",
            "Epoch 23/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0659 - val_loss: 0.3756\n",
            "Epoch 24/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.3417\n",
            "Epoch 25/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.3080\n",
            "Epoch 26/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.2762\n",
            "Epoch 27/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.2461\n",
            "Epoch 28/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.2181\n",
            "Epoch 29/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.1955\n",
            "Epoch 30/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.1723\n",
            "Epoch 31/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.1521\n",
            "Epoch 32/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.1346\n",
            "Epoch 33/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.1188\n",
            "Epoch 34/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.1038\n",
            "Epoch 35/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.0909\n",
            "Epoch 36/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.0803\n",
            "Epoch 37/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0548 - val_loss: 0.0712\n",
            "Epoch 38/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0543 - val_loss: 0.0633\n",
            "Epoch 39/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0538 - val_loss: 0.0571\n",
            "Epoch 40/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0534 - val_loss: 0.0520\n",
            "Epoch 41/150\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.0529 - val_loss: 0.0485\n",
            "Epoch 42/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.0459\n",
            "Epoch 43/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0447\n",
            "Epoch 44/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0443\n",
            "Epoch 45/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.0449\n",
            "Epoch 46/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.0464\n",
            "Epoch 47/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.0484\n",
            "Epoch 48/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.0506\n",
            "Epoch 49/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0499 - val_loss: 0.0534\n",
            "Epoch 50/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0495 - val_loss: 0.0565\n",
            "Epoch 51/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.0602\n",
            "Epoch 52/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.0633\n",
            "Epoch 53/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0669\n",
            "Epoch 54/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0483 - val_loss: 0.0690\n",
            "Epoch 55/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0716\n",
            "Epoch 56/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.0741\n",
            "Epoch 57/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.0757\n",
            "Epoch 58/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.0770\n",
            "Epoch 59/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0469 - val_loss: 0.0781\n",
            "Epoch 60/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.0794\n",
            "Epoch 61/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0804\n",
            "Epoch 62/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.0801\n",
            "Epoch 63/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.0799\n",
            "Epoch 64/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.0795\n",
            "Epoch 65/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.0792\n",
            "Epoch 66/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.0777\n",
            "Epoch 67/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.0772\n",
            "Epoch 68/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0757\n",
            "Epoch 69/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0753\n",
            "Epoch 70/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0734\n",
            "Epoch 71/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0723\n",
            "Epoch 72/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.0710\n",
            "Epoch 73/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.0695\n",
            "Epoch 74/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0676\n",
            "Epoch 75/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0662\n",
            "Epoch 76/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0641\n",
            "Epoch 77/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.0623\n",
            "Epoch 78/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0428 - val_loss: 0.0603\n",
            "Epoch 79/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0426 - val_loss: 0.0586\n",
            "Epoch 80/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.0570\n",
            "Epoch 81/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0422 - val_loss: 0.0557\n",
            "Epoch 82/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0420 - val_loss: 0.0542\n",
            "Epoch 83/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0531\n",
            "Epoch 84/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.0515\n",
            "Epoch 85/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.0503\n",
            "Epoch 86/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.0488\n",
            "Epoch 87/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0475\n",
            "Epoch 88/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0459\n",
            "Epoch 89/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0446\n",
            "Epoch 90/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0437\n",
            "Epoch 91/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.0430\n",
            "Epoch 92/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.0418\n",
            "Epoch 93/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0410\n",
            "Epoch 94/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0403\n",
            "Epoch 95/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0398\n",
            "Epoch 96/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0394\n",
            "Epoch 97/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.0389\n",
            "Epoch 98/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0383\n",
            "Epoch 99/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0379\n",
            "Epoch 100/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0375\n",
            "Epoch 101/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0372\n",
            "Epoch 102/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.0369\n",
            "Epoch 103/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0366\n",
            "Epoch 104/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0384 - val_loss: 0.0364\n",
            "Epoch 105/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0362\n",
            "Epoch 106/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.0360\n",
            "Epoch 107/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0359\n",
            "Epoch 108/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0358\n",
            "Epoch 109/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0357\n",
            "Epoch 110/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0356\n",
            "Epoch 111/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0355\n",
            "Epoch 112/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0356\n",
            "Epoch 113/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.0356\n",
            "Epoch 114/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0356\n",
            "Epoch 115/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0357\n",
            "Epoch 116/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0357\n",
            "Epoch 117/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.0358\n",
            "Epoch 118/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0359\n",
            "Epoch 119/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0360\n",
            "Epoch 120/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0362 - val_loss: 0.0361\n",
            "Epoch 121/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0361 - val_loss: 0.0362\n",
            "Epoch 122/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 0.0363\n",
            "Epoch 123/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0364\n",
            "Epoch 124/150\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.0357 - val_loss: 0.0365\n",
            "Epoch 125/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0367\n",
            "Epoch 126/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0369\n",
            "Epoch 127/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0368\n",
            "Epoch 128/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0370\n",
            "Epoch 129/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0370\n",
            "Epoch 130/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0372\n",
            "Epoch 131/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0373\n",
            "Epoch 132/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0375\n",
            "Epoch 133/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0375\n",
            "Epoch 134/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0376\n",
            "Epoch 135/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0379\n",
            "Epoch 136/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0376\n",
            "Epoch 137/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0379\n",
            "Epoch 138/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0377\n",
            "Epoch 139/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0376\n",
            "Epoch 140/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0377\n",
            "Epoch 141/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0375\n",
            "Epoch 142/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0376\n",
            "Epoch 143/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0378\n",
            "Epoch 144/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0376\n",
            "Epoch 145/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0376\n",
            "Epoch 146/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0375\n",
            "Epoch 147/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0374\n",
            "Epoch 148/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0375\n",
            "Epoch 149/150\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.0328 - val_loss: 0.0375\n",
            "Epoch 150/150\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_plot(test_one)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "BreXjQMHucxP",
        "outputId": "e9d0a27b-bf6a-4169-fe7a-8903aba1a966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.predict(first_try_x))\n",
        "\n",
        "nn_preds = model.predict(first_try_x)\n",
        "stat_preds = first_try_x[:,10:12]\n",
        "print(stat_preds[0:3,:])\n",
        "print(first_try_x[0:3,:])\n",
        "real_ys = first_try_y\n",
        "window = 1\n",
        "\n",
        "stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(window, real_ys, stat_preds, nn_preds)\n",
        "\n",
        "print(\"STAT TRAIN ENSEMBLE MSE: \", stat_avg_mse)\n",
        "print(\"NN TRAIN MODEL MSE: \", nn_mse)\n",
        "print(\"NN+STAT TRAIN ENSEMBLE MSE: \", nn_avg_mse)\n",
        "\n",
        "\n",
        "nn_preds2 = model.predict(first_try_x_val)\n",
        "stat_preds2 = first_try_x_val[:,10:12]\n",
        "print(stat_preds2[0:3,:])\n",
        "real_ys2 = first_try_y_val\n",
        "window2 = 1\n",
        "\n",
        "stat_avg_mse, nn_mse, nn_avg_mse = calculate_metrics(window, real_ys2, stat_preds2, nn_preds2)\n",
        "\n",
        "print(\"STAT VAL ENSEMBLE MSE: \", stat_avg_mse)\n",
        "print(\"NN VAL MODEL MSE: \", nn_mse)\n",
        "print(\"NN+STAT VAL ENSEMBLE MSE: \", nn_avg_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N1QzdPB5Xyf",
        "outputId": "2b6980c8-5f36-43e6-dd69-123b4ef4f1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 15ms/step\n",
            "[[-1.02839118 -1.00976611]\n",
            " [-1.02246525 -1.00636658]\n",
            " [-1.26201821 -1.25186096]]\n",
            "[[-1.18837558 -1.17737103 -1.121501   -1.17059781 -1.10056137 -1.18244965\n",
            "  -1.15197791 -1.10795973 -1.08849253 -1.09441329 -1.02839118 -1.00976611]\n",
            " [-1.01821843 -1.00215076 -0.98268356 -0.98776218 -1.01653933 -1.02669658\n",
            "  -1.04955297 -1.05378429 -1.02502265 -1.06055752 -1.02246525 -1.00636658]\n",
            " [-1.13081612 -1.13165825 -1.1214855  -1.13335285 -1.09780248 -1.07409879\n",
            "  -1.07071476 -1.14556118 -1.26370764 -1.20784277 -1.26201821 -1.25186096]]\n",
            "2\n",
            "STAT TRAIN ENSEMBLE MSE:  0.002580036183811661\n",
            "NN TRAIN MODEL MSE:  0.0034495044041024624\n",
            "NN+STAT TRAIN ENSEMBLE MSE:  0.0020458753035366454\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[[-0.36473955 -0.30719559]\n",
            " [ 0.03647174  0.04324496]\n",
            " [ 0.36326026  0.37759717]]\n",
            "2\n",
            "STAT VAL ENSEMBLE MSE:  0.002839849369788067\n",
            "NN VAL MODEL MSE:  0.008564990617821129\n",
            "NN+STAT VAL ENSEMBLE MSE:  0.0020829388950082527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data2 = cv_data[1:120]\n",
        "\n",
        "cv_data2[0:12] = np.array([0,1,2,3,4,5,6,7,8,9,10,11]).reshape([12,1])\n",
        "output = stat_true_window_cross_val(cv_data2, ['auto_arima','complex_smoothing'], [new_aaPredFunction, new_cesPredFunction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "HFHezSU2AQPb",
        "outputId": "6d44c730-96f5-4fd5-dd61-522af7816b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cv_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c366178482a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv_data2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat_true_window_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_aaPredFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cesPredFunction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0:3,0:13])\n",
        "print(cv_data[0:12])"
      ],
      "metadata": {
        "id": "CMN8rUW_EsM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testResultsQQQ, trainSeriesQQQ, testSeriesQQQ = prep_ticker('QQQ')\n",
        "\n",
        "print(testResultsQQQ.head() )\n",
        "print(trainSeriesQQQ.head() )\n",
        "print(testSeriesQQQ.head() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjvXcXzlkHvq",
        "outputId": "13b0e978-c764-4595-8710-4efa008f77f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price  auto_arima  dyn_theta  auto_ets  complex_smoothing\n",
            "0  376.239990           0          0         0                  0\n",
            "1  377.880005           0          0         0                  0\n",
            "2  378.390015           0          0         0                  0\n",
            "3  378.799988           0          0         0                  0\n",
            "4  379.730011           0          0         0                  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testResultsQQQ, trainSeriesQQQ, testSeriesQQQ = prep_ticker('QQQ', model_list = ['auto_arima', 'dyn_theta', 'complex_smoothing'])\n",
        "\n"
      ],
      "metadata": {
        "id": "sLT6_TwVmgc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "3dba41b7-f16b-48c2-cc10-faf439cf784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'yf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-34105c547167>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestResultsQQQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSeriesQQQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSeriesQQQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QQQ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dyn_theta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-69f3cddda564>\u001b[0m in \u001b[0;36mprep_ticker\u001b[0;34m(ticker, start_date, end_date, intervals, train_size, model_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprep_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2022-06-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2023-09-30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintervals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'60m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto_arima'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dyn_theta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auto_ets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complex_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m#print(data.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qqqpreds, qqqerrors = stat_test_wrapper(trainSeriesQQQ, testSeriesQQQ, testResultsQQQ, 200, 10)\n"
      ],
      "metadata": {
        "id": "5g9CjbLhshfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}